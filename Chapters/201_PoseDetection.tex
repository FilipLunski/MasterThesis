\chapter{Výběr algoritmu pro detekci pózy}
\label{chap:Pose}

Jelikož je dnes dostupných mnoho různých algoritmů či natrénovaných modelů pro
detekci pózy osob v obrázku či videu, nemá smysl pro naše řešení implementovat
takovýto algoritmus od nuly. Možné by to samozřejmě bylo, i vzhledem k
dostupnosti otevřených trénovacích dat (např. dataset COCO \cite{coco}),
nicméně bychom pravděpodobně nedosáhli kvalitních výsledků, jako řešení, která
jsou výsledkem mnoholetých výzkumů. Hlavně pak bychom těžko dosáhli výkonů
těchto řešení, a ten je pro nás stěžejní, jelikož potřebujeme video zpracovávat
v reálném čase.

V následující kapitole budou popsány obecné principy detekce osob a jejich pózy
v obraze. Následně budou popsány některé populární algoritmy pro detekci pózy
se zaměřením na jejich specifika. Několik z nich pak bude otestováno, výsledky
budou porovnány, a na jejich základě bude zvolen algoritmus použitý v konečném
řešení detekce pádu.

\section{Detekce pózy}

\begin{figure}[]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{Figures/keypoints.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=0.35\textwidth]{Figures/pose1.png}
    \end{minipage}
    \caption{(Vlevo) Topologie klíčových bodů použitá např. v COCO-pose.\cite{2dhpe} (Vpravo) Příklad detekce pózy pomocí YOLO.}
    \label{fig:keypoints}
\end{figure}

Úloha detekce pózy spočívá v nalezení klíčových bodů postavy v obraze. Může se
jednat také o zvíře, v našem případě se ale budeme zabývat pouze klíčovými body
lidské postavy. Klíčové body představují důležité body lidského těla, znalost
jejich lokalizace nám umožňuje analyzovat pózu dané osoby, popřípadě sledovat
její pohyb. K základním klíčovým bodům patří hlava, ramena, lokty, zápěstí,
kyčle, kolena a kotníky, viz obrázek \ref{fig:keypoints}. Některé algoritmy dokážou
rozeznat i orientaci dlaně či stopy, nebo rozpoznat klíčové body na hlavě, jako
jsou ústa, nos, oči a uši \cite{blazepose}.

Klíčové body jsou většinou reprezentovány jako dvojice souřadnic $(x, y)$
vzhledem k celému obrazu, některé algoritmy poskytují i souřadnice
normalizované vzhledem k bounding boxu osoby. Existují také algoritmy pro 3D
souřadnice, těmito se ale nebudeme zabývat, i když by mohly stanovit zajímavou
alternativu, zejména pokud by pro detekci bylo použito více kamer z různých
pohledů.

V oblasti algoritmů pro detekci pózy existují dva základní přístupy: zdola
nahoru a shora dolů. Přístup zdola nahoru se snaží detekovat všechny klíčové
body v obraze, aniž by rozlišoval jednotlivé osoby, pokud je algoritmus schopen
detekce pózy pro více osob, pak v dalším kroku tyto body spojuje do
jednotlivých postav. Naproti tomu přístup shora dolů nejprve detekuje všechny
osoby v obraze, v jejich rámci pak detekuje klíčové body.

\section{Detekce klíčových bodů}

\subsection{Heatmapy}

U obou výše zmíněných přístupů se nejčastěji provádí vyhledání všech klíčových
bodů pomocí tzv. heatmap. Je to 2D mapa pravděpodobnosti, že se v daném bodě
vyskytuje nějaký klíčový bod. Maximální hodnoty v této mapě pak představují
lokalizaci klíčových bodů.

Pro vygenerování heatmap se používá konvoluční neuronová síť. Pro každý klíčový
bod, resp. pro každý typ klíčového bodu k (v případě detekce pózy více osob)
vzniká jedna heatmapa. Jako referenční heatmapy pro trénování se používají
mapy, kde je klíčový bod reprezentován 2D Gaussovým rozložením s vrcholem v
místě daného bodu.

V dalším kroku jsou z heatmap vygenerovány, nejčastěji s pomocí algoritmu
argmax, souřadnice klíčových bodů. V případě vícero osob je pak třeba tyto body
spojit do jednotlivých osob.

\subsection{Regrese}

Využití heatmap je velmi přesné, nicméně z důvodu nutnosti provádění dvou
sekvenčních výpočtů je taky trochu pomalé. Taky komplikují proces trénování,
jelikož musíme spolu s trénovacími daty dodat modelu i heatmapy. Některé
algoritmy se proto snaží formulovat úlohu jako regresi vedoucí přímo k
souřadnicím klíčových bodů. Tento přístup je ve své podstatě trochu méně
přesný, nicméně je rychlejší.

Vůbec první algoritmus pro detekci pózy využívající hluboké učení, DeepPose
\cite{deeppose}, který byl vytvořen v roce 2014 společností Google, používal
právě regresi. Také algoritmus YOLO používá regresi pro určení souřadnic
klíčových bodů, nicméně detekce je prováděná pro detekované objekty, nikoliv
nad celým vstupním obrazem \cite{yolo-pose}.

% , a je součástí poupraveného modelu pro detekci objektů - v posledních
% vrstvách sítě je kromě regrese definující bounding box a klasifikace určující
% třídu prováděna regrese pro určení klíčových bodů.

\section{Detekce objektů a osob v obraze}
\label{sec:obj_det}

Detekce osob se v podstatě může generalizovat na detekci objektů v obraze.
Detekci objektů v obraze definujeme jako úlohu, kdy ve vstupním obrázku určíme
lokaci a třídu všech hledaných objektů.

V kapitole \ref{chap:CNN} jsme si popsali základní architekturu konvolučních
neuronových sítí, ta se ale většinou v praxi používá pro klasifikaci obrázků,
nikoliv pro detekci objektů - algoritmus tedy pouze určí, o jakou třídu objektu
se jedná, a ideálně potřebuje, aby objekt vyplňoval celý vstupní obraz.
Teoreticky by bylo možné detekci formulovat jako regresní problém a natrénovat
takovou síť, která by pomocí několika konvolučních vrstev následovaných
několika plně propojenými vrstvami byla schopna predikovat lokalizaci a třídu
všech objektů v obraze \cite{szegedy}. Problém detekce je ale velice komplexní
a taky by vyžadoval velice komplexní síť - více vrstev s mnoha filtry, resp.
neurony. Jak již ale bylo zmiňováno, komplexnost sítě zvyšuje její nároky na
výpočetní výkon a komplikuje nebo úplně znemožňuje její trénování s ohledem na
pravděpodobnost přetrénování.

Snahou tedy je najít metody, které poupraví funkčnost sítě tak, aby byla
schopna efektivní detekce objektů. Většina těchto metod se nějakým způsobem
snaží rozdělit vstupní obrázek na menší části, ty následně jednak klasifikovat,
a tedy určit, zda se v dané lokalitě vyskytuje objekt, popřípadě pomocí regrese
určit jeho přesnou lokalizaci. Lokalizace je většinou reprezentována jako
souřadnice obdélníku ohraničujícího daný objekt, tzv. bounding box. Rozdělení
může být provedeno přímo na vstupním obrázku nebo na mapě příznaků v rámci
sítě. Taky můžeme buď rozdělit obrázek na pevně dané oblasti (např. do mřížky)
- jednofázový přístup, anebo v jedné fázi předpřípravit množinu oblastí a ve
druhé fázi nad těmito oblastmi provést klasifikaci a regresi - dvoufázový
přístup.

\subsection{Sliding window}

Jednou z prvních takových metod byl tzv. sliding window (klouzavé okno), který
aplikuje hrubou sílu. Vstupní obrázek se postupně projíždí oknem o fixní
velikosti. Vznikne tak množina pokrývající každou možnou lokaci objektů. Na
tyto oblasti se pak aplikuje klasifikační algoritmus. Postup se opakuje pro
několik velikostí okna, aby se detekovaly objekty různé velikosti.

Tento postup je ale velice pomalý, jelikož je pro každý obrázek zvolený velký
počet oblastí, pro které je třeba provést klasifikaci popřípadě regresi. Navíc
je většina těchto oblastí prázdná, a dochází tak k plýtvání výpočetním výkonem.
Algoritmus se taky potýká s překrývajícími se objekty.

Další metody se tedy snaží redukovat počet oblastí, na které se aplikuje
klasifikace, tak, že se vybere pouze oblasti, které pravděpodobně budou
obsahovat nějaký objekt.

\subsection{Dvoufázový přístup}

\subsubsection*{R-CNN}
Prvním algoritmem, který efektivně zredukoval počet oblastí pro klasifikaci,
byl algoritmus R-CNN (Region-based Convolutional Network) \cite{r-cnn}. Tento
algoritmus nejprve použil některou z dostupných metod (autoři použili selective
search) pro vygenerování navržených oblastí (region proposals), které
pravděpodobně obsahují nějaký objekt. Tyto metody jsou nezávislé na třídě
objektů. Algoritmus tedy vygeneruje zhruba 2000 oblastí, vzniklé obrázky jsou
následně upraveny na velikost požadovanou CNN v další fázi. CNN extrahuje z
dané oblasti mapu příznaků, na její základě plně propojené vrstvy predikují
třídu objektu popřípadě jeho bounding box.

Problémem R-CNN je, že výběr oblasti a jejich následná klasifikace jsou
nezávislé úlohy a jsou nezávisle trénovány. Detekce objektu je taky poměrně
pomalá, protože je extrakce příznaků prováděná pro všechny oblasti zvlášť. Tyto
problémy se snaží řešit další upravené verze R-CNN.

\subsubsection*{Fast R-CNN}
První z nich je Fast R-CNN \cite{fast-r-cnn}, která je upravená tak, aby bylo
možné provádět trénování v jednom kroku. Taky extrahuje příznaky pro celý
vstupní obraz najednou, pomocí selective search pak identifikuje oblasti zájmu
(ang. region of interest - RoI), které následně použije pro klasifikaci a
regresi. Tato metoda je přesnější a asi desetkrát rychlejší než původní R-CNN.

\subsubsection*{Faster R-CNN}
Další algoritmus, Faster R-CNN \cite{faster-r-cnn}, nahrazuje metodu selective
search vlastní, plně konvoluční sítí RPN (region proposal network).
Zefektivňuje tak proces trénování, výsledná síť je také rychlejší a přesnější
než Fast R-CNN.

\subsection{Jednofázový přístup}

Jednofázový přístup se snaží najít řešení, ve kterém není nutné hledat navržené
oblasti, ale provést klasifikaci a regresi na předem dané množině oblastí,
obvykle určené mřížkou.

\subsubsection*{YOLO}
Prvním takovým algoritmem byl YOLO (taky YOLOv1, z ang. you only look once)
\cite{yolo}. Ten, v původní verzi, rozdělí vstupní obraz do pevně dané mřížky
velikosti $S \times S$ a v každém z těchto polí určí $B$ bounding boxů a jejich
třídu. V původní verzi bylo zvoleno $S = 7$ a $B = 2$.

Obraz je nejprve zpracován pomocí konvolučních vrstev, které extrahují mapu
znaků o velikosti $S \times S \times K$, kde $K$ je počet kanálů. Každý pixel
této mapy představuje jedno pole mřížky. Dále je mapa zpracována plně
propojenými vrstvami, které provádějí nad každým polem mřížky klasifikaci a
regresi, viz obrázek \ref{fig:yolo}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/yolo}
    \caption{Architektura původní verze YOLO \cite{yolo}}
    \label{fig:yolo}
\end{figure}

Každý bounding box je reprezentován souřadnicemi středu a velikosti (šířka a
výška). Dohromady s informací o jistotě detekce bounding boxu (confidence
score) vrátí model $5$ informací o každém bounding boxu. Pro každé pole mřížky
pak určí společnou informaci o třídě všech objektů v daném poli. Pokud objekt
není detekován, třída indikuje pozadí a souřadnice bounding boxu jsou
ignorovány. Velikost výstupního vektoru je tedy $7 \times 7 \times (2 * 5 +
    C)$, kde $C$ je počet definovaných tříd - v původní verzi pouze 20.

Tento algoritmus, navržený v roce 2015 J. Redmonem et al., byl revolučně
rychlý, zároveň v porovnání s jinými real-time detekčními algoritmy dosahoval i
slušné přesnosti. Nicméně byl velice citlivý na velikost objektu a přesnost
detekce, zejména u menších objektů, byla horší než u dvoufázových algoritmů.

Další verze algoritmu YOLO přinesly postupná vylepšení ve formě optimalizace
trénování a architektury. YOLOv2 \cite{yolo9000} zavedl mj. trénování na
několika měřítkách a byl natrénován s 9000 třídami (proto taky nazýván
YOLO9000). YOLOv3 \cite{yolov3} přinesl mj. detekci na několika měřítkách.
Postupně byla taky zvětšována mřížka a měnila se použitá architektura CNN sítě
sloužící pro extrakci příznaků pro jednotlivá pole mřížky. Postupně taky byly
přidávány další funkce jako je segmentace, detekce pózy či sledování objektů
(ang. tracking).

V 2020 roce firma Ultralytics poprvé implementovala YOLO s využitím populární
knihovny PyTorch (YOLOv5), což umožnilo snadnější využití YOLO v praxi. Firma
Ultralytics taky vytvořila framework pro použití různých verzí YOLO (YOLOv3 a
novější). Taky pracuje na dalších vylepšeních a optimalizacích. Konkrétně
vytvořila YOLOv5 (2020), YOLOv8 (2023) a YOLOv11 (2024). Tyto verze nicméně
nejsou podloženy odbornými články, někteří je tak považují za neoficiální
verze.

\subsubsection*{SSD}
Dalším populárním algoritmem, který používá jednofázový přístup, je SSD (z ang.
single shot detector) \cite{szegedy:ssd}. Ten rozdělí vstupní obraz do několika
mřížek o různé velikosti. Postup je takový, že nejprve projde obraz konvoluční
síťí, konkrétně síťí VGG16, která extrahuje mapu příznaků. Tu se postupně
dalšími konvolučními vrstvami zmenšuje, výstup každého stádia zmenšení,
reprezentující mřížku dané velikosti, se spolu s původní mapou dále zpracovává
plně propojenou sítí.
\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/ssd.png}
    \caption{Architektura SSD \cite{szegedy:ssd}}
    \label{fig:ssd}
\end{figure}

Výstupní bounding boxy nejsou, jako v případě YOLO, pouze výsledkem regrese,
ale pro každé pole dané mřížky je definováno několik výchozích oblastí (ang.
default box), ze kterých jsou vybrány ty, které obsahují objekt. K ním je
predikována třída objektu a posun i změna velikosti výchozí oblasti,
upřesňující výsledný bounding box.

V době svého vzniku byl SSD rychlejší a přesnější než YOLO, ale novější verze
YOLO jej už předběhly. Nicméně některé principy SSD, jako výchozí oblasti či
použití různých měřítek, byly převzaty do novějších verzí YOLO.

\section{Charakteristiky vybraných implementací pro detekci pózy}

V této sekci bude popsáno několik populárních algoritmů (a jejích implementací)
pro detekci pózy zejména se zaměřením na jejich rychlost, přesnost a specifika
architektury.

Velká část algoritmů byla zamítnutá a neproběhlo ani jejích testování.
Nejčastějším důvodem zamítnutí některého z algoritmů je, že schází jeho volně
dostupná, aktualizovaná implementace. Tvůrci algoritmů většinou investují čas a
zdroje pro vývoj algoritmu a natrénování modelu (často pro akademické účely),
pak ale neinvestují do jeho údržby. Zejména v prostředí Pythonu, kde se
neustále vyvíjejí nové knihovny a zpětná kompatibilita starších verzí není
zaručena, je pak obtížné použít takovéto řešení ve svém projektu, aniž by bylo
nutné investovat další čas do pochopení zdrojového kódu a jeho úpravy. Jak již
bylo zmíněno na počátku kapitoly, implementace algoritmu od nuly by vyžadovala
velké množství času a zdrojů (zejména výpočetních), hlavně by taky byla
potřebná hlubší znalost problematiky. Někdy je možné řešení použít za cenu
kompromisu ve formě použití starších verzí knihoven či Pythonu, může to ale
představovat bezpečnostní rizika. Taky některé algoritmy jsou sice volně
dostupné, ale jejich implementace jsou součástí komerčních produktů. %todo citace

\subsection{DeepPose}
DeepPose je historicky první algoritmus pro detekci pózy využívající hluboké
učení. Vyvinuli jej Alexander Toshev a Christian Szegedy ze společnosti Google
v roce 2014 \cite{deeppose}. Algoritmus předpokládá, že se ve vstupním obraze
nachází pouze jedna osoba. Síť se snaží v jednom kroku pomocí regrese jak
detekovat osobu, tak i její klíčové body. Jelikož je těžké takto dosáhnout
velmi přesných výsledků, algoritmus používá další fázi, která pomocí regrese
provádí posun bodů k přesnějším výsledkům. Tato fáze je aplikována opakovaně,
kaskádně se tak zvyšuje přesnost detekce.

Při svém vzniku byl DeepPose revoluční, nicméně v porovnání s dnešními řešeními
je poměrně pomalý a nepřesný. Nicméně položil základ pro využití hlubokého
učení v oblasti detekce pózy.

\subsection{OpenPose}

OpenPose \cite{openpose} je typicky příklad přístupu zdola nahoru. Jeho výhodou
je ale možnost vyhledání více osob v jednom snímku. Tento algoritmus, který
vyvinuli v roce 2019 Zhe Cao et al., nejprve pomocí CNN vytvoří heatmapu pro
každý typ klíčového bodu. Pro spojení bodů do jednotlivých osob využije pole
propojení klíčových bodů (ang. part affinity field - PAF). PAF je mapa
vytvořená pro každou končetinu (myšleno obecně spojení dvou klíčových bodů),
která v oblasti dané končetiny obsahuje hodnoty určující směr z jednoho bodu do
druhého. Pokud pak dáme do hromady informace z heatmap a z PAF, jsme schopni
poměrně jednoznačně zkompletovat jednotlivé klíčové body do celých postav.
Stejně jako heatmapy, jsou i PAF součástí trénovacích dat.

\subsection{OpenPifPaf}

Algoritmus OpenPifPaf \cite{openpifpaf}, vyvinutý v roce 2021 Svenem Kreissem
et al., je v podstatě vylepšenou verzí OpenPose. Jeho název je odvozen od dvou
stavebních kamenů: PIF (Part Intensity Field) - pole intenzity klíčových bodů,
a PAF (Part Affinity Field) - pole propojení klíčových bodů. PIF je rozšířením
heatmap, kdy kromě intenzity pravděpodobnosti klíčového bodu obsahuje i jeho
posun, zaručující přesnější lokalizaci bodu, a odhadovanou velikost dané části
těla. PAF v OpenPifPaf je taky podobný tomu v OpenPose, navíc ale indikuje
kromě směru i velikost dané končetiny, což umožňuje lepší prostorové zachycení
pózy.

Dalším rozšířením oproti OpenPose je možnost sledování osob ve videu. Mapa
příznaků, která je výsledkem vstupní CNN, je udržována v mězipaměti, do další
části sítě pak vždy vstupují mapy pro aktuální a předchozí snímek. Výstupem pak
kromě klíčových bodů v každém snímku a jejich propojení tvořící kostru, jsou i
propojení mezi klíčovými body z jednotlivých snímků, viz
\ref{fig:pipaf-tracking}. Algoritmus si pak udržuje ID sledovaných osob, pokud
k dříve nalezené osobě je nalezena nová pozice, je jí přiřazeno stejné ID.
Pokud je nalezena nová osoba, je jí přiřazeno nové ID.

\begin{figure}[]
    \centering
    \includegraphics[height=0.2\textheight]{Figures/skeleton_forward2.png}
    \caption{Vizualizace  sledování osoby mezi dvěma snímky v OpenPifPaf \cite{openpifpaf}}
    \label{fig:pipaf-tracking}
\end{figure}

U OpenPifPaf je k dispozici výběr několika páteřních modelů jako je ResNet50 či
ShuffleNet v různých variantách a velikostech. Můžeme tak zvolit model, který
je kompromisem mezi výkonem a přesností, v závislosti na konkrétních
požadavcích aplikace.

\subsection{MediaPipe - BlazePose}

MediaPipe je framework vyvinutý společností Google, umožňující jednoduchou
integraci různých technik strojového učení. Obsahuje různé algoritmy pro řešení
úloh jako detekce objektů, segmentace či detekce klíčových bodů (tváře či
pózy). MediaPipe je optimalizován pro mobilní zařízení a webové aplikace.
Detekce pózy v tomto frameworku je postavená na algoritmu BlazePose.

BlazePose implementuje přístup shora dolů, detekuje tedy nejprve RoI, ve
kterých detekuje osobu a její pózu. Nativně podporuje pouze jednu osobu ve
snímku. Ve videu ale v rámci optimalizace neprovádí detekci RoI pro každý
snímek, pouze pokud v aktuální RoI již není detekována osoba. Výhodou tohoto
algoritmu je, že detekuje 33 bodů v postavě, což je podstatně více, než většina
ostatních algoritmů, umožňuje tak přesnější analýzu některých situací, např.
podle natočení tváře, dlaní či stop.

Framework MediaPipe implementuje BlazePose spolu s detekcí více osob (použije v
první fáze detektor objektů) i sledování. Výhodou tohoto frameworku je jeho
kontinuální vývoj a jednoduchost integrace. Nevýhodou ale je, že pro systémy
Windows není implementována podpora GPU. Jelikož náš výsledný produkt bude
spouštěn primárně na Windows zařízeních, je pro nás tato vlastnost rozhodující.
Model je dostupný v třech velikostních variantách: $Lite$, $Full$ a $Heavy$.

\subsection{YOLO}

Od vydání YOLOv7 v roce 2022 integruje framework YOLO i detekci pózy. Oficiální
článek Chien-Yao Wanga et al. \cite{yolov7}. sice neobsahoval tuto funkčnost,
ale oficiální implementace zahrnula i implementaci YOLO-Pose \cite{yolo-pose}.
Obecně detekce pózy v YOLO kombinuje přístup shora dolů a zdola nahoru.
Algoritmus sice vyhledává klíčové body spolu s bounding boxy osob, nicméně vše
v jednom kroku. Samotná detekce klíčových bodů využívá regresi, což
zjednodušuje proces trénování, jelikož není třeba tvořit heatmapy.

Architektura použita v YOLO-Pose se ale liší od architektury používané v
pozdějších verzích. V YOLO-Pose jsou na konci řetězce umístěny hlavy pro různá
měřítka, jejich výstupem jsou bounding boxy a klíčové body, oba tvořené spolu.
V pozdějších verzích je architektura YOLO koncipována universálněji pro různé
úlohy. Obsahuje tak tři fáze\cite{yolov11}: páteř (ang. backbone), která
extrahuje mapu příznaků, krk (ang. neck), který přizpůsobuje mapu příznaků pro
různá měřítka, a hlavy (ang. head), které paralelně zpracovávají výstupy pro
různé úlohy, viz obrázek \ref{fig:yolov11} . Bounding box a klíčové body jsou
tedy sice generovány paralelně a teoreticky nezávisle, nicméně s ohledem na
proces trénování a postprocessing se v praxi navzájem výrazně ovlivňují.

\begin{figure}[]
    \centering
    \includegraphics[height=0.2\textheight]{Figures/yolo_v11.png}
    \caption{Architektura YOLOv11 \cite{yolov11}}
    \label{fig:yolov11}
\end{figure}

Nejnovější verze YOLO taky podporují kombinaci detekce klíčových bodů a
sledování osob. Model tedy kromě klíčových bodů vrací ID dané osoby, pomocí
kterého můžeme spojit danou postavu s předchozími snímky. Můžeme tak efektivně
analyzovat pohyby jednotlivých osob.

Jednou z výhod framevorku YOLO, zejména verzí vyvíjených firmou Ultralytics, je
široká škála velikosti modelů. Každý model je dostupný v pěti variantach:
$Nano$, $Small$, $Medium$, $Large$, $Xlarge$.

\subsection{Torchvision}

Torchvision je knihovna, která je součástí frameworku PyTorch. Obsahuje různé
nástroje pro strojové vidění, jako je detekce objektů či segmentace. Její
součástí je i předtrénovaný model pro detekci pózy, který implementuje
algoritmus Keypoint R-CNN \cite{keypoint-rcnn}.

Algoritmus Keypoint R-CNN je založen na stejné myšlence jako Mask R-CNN
\cite{mask-r-cnn}, a tedy rozšíření Faster R-CNN o další hlavu, která v případě
Mask R-CNN provádí segmentaci, v případě Keypoint R-CNN detekuje klíčové body.
V algoritmu je taky upravená pooling vrstva, která zajišťuje, že se výstupy
algoritmu shodují se vstupy s přesností pixelu.

Tuto implementaci budeme testovat zejména z důvodu její jednoduchosti použití a
integrace do frameworku PyTorch, který budeme používat i v další části řešení.
Na druhou stranu máme oproti jiným framevorkům, jako je YOLO či MediaPipe, k
dispozici pouze jeden model, nikoliv více více velikostních variant.

\section{Testování a porovnání vybraných algoritmů pro detekci pózy}

Pro testování jsme vybrali čtyři algoritmy pro detekci pózy, zejména na základě
jejich jednoduchosti implementace, aktualizované podpory a požadovaných funkcí.
Budeme tedy testovat algoritmy Torchvision, OpenPifPaf, MediaPipe BlazePose a
YOLO v nejnovější verzi 11. Testy byly provedeny na 25 videích ze stejného
datasetu jako byl použit později pro trénování algoritmu pro analýzu klíčových
bodů. Testování probíhalo na počítači s procesorem Intel Core i7, 32 GB RAM a
grafickou kartou NVIDIA GeForce RTX 3080 s 10 GB VRAM.

Algoritmy Torchvision a OpenPifPaf byly testovány s použitím GPU, MediaPipe
pouze s využitím CPU, jelikož nemá podporu GPU v prostředí Windows. Algoritmus
YOLO jsme testovali na GPU a CPU, abychom porovnali jeho výkon v různých
podmínkách. Algoritmus YOLO jsme taky vyzkoušeli na GPU s využitím funkce
sledování.

Výstupem testování pro daný algoritmus a jeho variantu je průměrná doba
zpracování jednoho snímku a videa s vykreslením klíčových bodů. Tyto videa nám
dovolují ověřit schopnost detekce klíčových bodů v různých situacích a její
přesnost.

Podíváme se nyní na výsledky testování jednotlivých algoritmů. Jelikož všechny
algoritmy kromě Torchvision Keypoint R-CNN mají několik variant, porovnáme
jednotlivé varianty s ohledem na rychlost a přesnost. Pro každý algoritmus pak
vybereme variantu optimální pro naše využití. Nakonec porovnáme mezi sebou
algoritmy ve vybraných variantách a zvolíme model, který použijeme v další
části vývoje.

\subsection{Výkonové požadavky}
Při výběru algoritmu musíme s ohledem na práci v reálném čase brát v úvahu
zejména výkon detekčního algoritmu. Bezpečnostní kamery mají obvykle snímkovou
frekvenci od 15 do 30 snímku za sekundu (FPS), ideální by tedy bylo, aby
konečný program byl schopen pracovat s frekvencí alespoň 30 FPS. Zároveň se
předpokládá, že v prostředí, kde bude program nasazen, bude dostupná grafická
karta.

V této fázi jsme již zkoušeli trénování neuronové sítě pro klasifikaci pózy, a
ověřili jsme, že i v případě hlubších a komplexnějších sítích dosahujeme doby
interference v řádu nižších jednotek milisekund. Hlavní vliv na výslednou
rychlost programu tedy bude mít hlavně detekční algoritmus, který musí
zpracovávat mnohem větší objem dat - stovky tisíc až miliony pixelů oproti
např. 17 klíčovým bodům v případě klasifikačního algoritmu.

\subsection{OpenPifPaf}

Algoritmus OpenPifPaf jsme zkoušeli v několika variantách, postavených na síti
\textit{ResNet 50} a \textit{ShuffleNet V2} \cite{shufflenetv2}. V tabulce
\ref{tab:openpifpaf_performance} vidíme, že většina variant ani zdaleka
nedosahuje požadovaného výkonu. Taky tento algoritmus dosahuje nejhorších
kvalitativních výsledků ze všech testovaných algoritmů. Kromě nedetekování
části těla, které nejsou vidět, totiž často nedetekuje člověka vůbec,
nejčastěji pak když člověk padá nebo leží, což jsou situace pro nás stěžejní.
Hlavně tento problém vystupuje ve variantě $resnet50$ a $shufflenetv2k16$, jsou
tak pro nás nepoužitelné. Varianty $shufflenetv2k30$ a $tshufflenetv2k30$ by
sice s ohledem na kvalitu výsledků použitelné byly, nicméně je jejich výkon
příliš nízký.

\begin{table}[htbp]
    \centering
    \caption{Porovnání výkonu modelu OpenPifPaf}
    \label{tab:openpifpaf_performance}
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Verze}   & \textbf{Interference [ms]} & \textbf{Frekvence [FPS]} \\
        resnet50         & 49,2                       & 20.32393576103568        \\ \hline
        shufflenetv2k16  & 31,3                       & 31.910143270071657       \\ \hline
        shufflenetv2k30  & 58,6                       & 17.069978743147114       \\ \hline
        tshufflenetv2k30 & 70,0                       & 14.280550155929461       \\ \hline
    \end{tabular}
\end{table}

\subsection{MediaPipe BlazePose}

Algoritmus BlazePose z knihovny MediaPipe jsme testovali ve třech variantách:
$Lite$, $Full$ a $Heavy$. Ve všech variantách tento algoritmus dosahoval velmi
kvalitních výsledku, asi nejlepších ze všech testovaných algoritmů. Taky s
ohledem na to, že nemáme k dispozici grafickou akceleraci, je jeho výkon velmi
dobrý, viz tabulka \ref{tab:mediapipe_performance}. Verze $Lite$ a $Full$ by
tak mohla být v našem řešení použitelná, což by nám taky dávalo možnost
nasazovat výsledný program v mobilních zařízeních či jiných systémech bez
grafické karty.

\begin{table}[htbp]
    \centering
    \caption{Porovnání výkonu modelu MediaPipe BlazePose}
    \label{tab:mediapipe_performance}
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Verze} & \textbf{Interference [ms]} & \textbf{Frekvence [FPS]} \\
        \hline
        lite           & 25.5                       & 39.23930953748518        \\ \hline
        full           & 30.9                       & 32.40481233370272        \\ \hline
        heavy          & 68.2                       & 14.654971132094836       \\ \hline
    \end{tabular}
\end{table}

\subsection{Torchvision Keypoint R-CNN}

Torchvision Keypoint R-CNN nedosáhla ani dostatečného výkonu, viz tabulka
\ref{tab:torchvision_performance}, ani kvalitních výsledků. Podobně jako
\textit{OpenPifPaf} má totiž problém, když osoba padá anebo leží. V tomto
případě osobu často detekuje, ale naprosto ztrácí přesnost detekovaných klíčových
bodů, nejčastěji záměnou jednotlivých bodů, viz obrázek \ref{fig:torchvision_bad}. 

\begin{figure}[]
    \centering
    \includegraphics[width=0.2\textwidth]{Figures/torchvision_bad.png}
    \caption{Příklad nepřesné detekce bodů v modelu Torchvision Keypoint R-CNN}
    \label{fig:torchvision_bad}
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Výkon modelu Torchvision Keypoint R-CNN}
    \label{tab:torchvision_performance}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Interference [ms]} & \textbf{Frekvence [FPS]} \\
        \hline
        50.3                       & 19.897253813962013       \\ \hline
    \end{tabular}
\end{table}

% \begin{table}[htbp]
%     \centering
%     \caption{}
%     \label{}
%     \begin{tabular}{|l|l|l|l|}
%         \hline
%         \textbf{Model} & \textbf{Verze}            & \textbf{Interference [ms]} & \textbf{Frekvence [FPS]} \\
%         \hline
%         mediapipe      & lite                      & 25.5                     & 39.23930953748518        \\ \hline
%         mediapipe      & full                      & 30.9                     & 32.40481233370272        \\ \hline
%         mediapipe      & heavy                     & 68.2                     & 14.654971132094836       \\ \hline
%         openpifpaf     & resnet50                  & 49.2                     & 20.32393576103568        \\ \hline
%         openpifpaf     & shufflenetv2k16           & 31.3                     & 31.910143270071657       \\ \hline
%         openpifpaf     & shufflenetv2k16-apollo-24 & 32.0                      & 31.293453481612836       \\ \hline
%         openpifpaf     & shufflenetv2k16-apollo-66 & 38.4                     & 26.046095149101856       \\ \hline
%         openpifpaf     & shufflenetv2k16-wholebody & 51.5                     & 19.405343193400522       \\ \hline
%         openpifpaf     & shufflenetv2k16-withdense & 31.9                     & 31.3077395791324         \\ \hline
%         openpifpaf     & shufflenetv2k30           & 58.6                     & 17.069978743147114       \\ \hline
%         openpifpaf     & shufflenetv2k30-apollo-66 & 66.1                     & 15.122976213182007       \\ \hline
%         openpifpaf     & shufflenetv2k30-wholebody & 80.1                     & 12.481867012174858       \\ \hline
%         openpifpaf     & tshufflenetv2k30          & 70.0                       & 14.280550155929461       \\ \hline
%         yolo           & cpu nano                  & 32.5                     & 30.74898266318138        \\ \hline
%         yolo           & cpu small                 & 53.9                     & 18.563280918148752       \\ \hline
%         yolo           & cpu medium                & 114.1                     & 8.763492212102856        \\ \hline
%         yolo           & cpu large                 & 143.4                     & 6.973250908805735        \\ \hline
%         yolo           & cpu xlarge                & 833.2                     & 1.2002117851080778       \\ \hline
%         yolo           & gpu nano                  & 15.1                     & 66.32336074356859        \\ \hline
%         yolo           & gpu small                 & 15.2                     & 65.9717806367005         \\ \hline
%         yolo           & gpu medium                & 17.4                     & 57.5001504688788         \\ \hline
%         yolo           & gpu large                 & 24.4                     & 41.025916403472124       \\ \hline
%         yolo           & gpu xlarge                & 24.4                     & 41.00455404907248        \\ \hline
%         yolo           & gpu track nano            & 27.4                     & 36.49993432946052        \\ \hline
%         yolo           & gpu track small           & 27.7                     & 36.14849744520849        \\ \hline
%         yolo           & gpu track medium          & 29.5                     & 33.88226705742545        \\ \hline
%         yolo           & gpu track large           & 37.5                     & 26.663753062433376       \\ \hline
%         yolo           & gpu track xlarge          & 40.5                     & 24.69518323409046        \\ \hline
%         torchvision    & -                         & 50.3                     & 19.897253813962013       \\ \hline
%     \end{tabular}
% \end{table}

% openpifpaf, mediapipe(shufflenet, resnet), torchvision, yolo

\endinput