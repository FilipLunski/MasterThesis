\chapter{Klasifikace pózy}
\label{sec:PoseClassification}

Problematika vyhodnocování je velmi široká a přináší mnoho problémů. Ostatně i
člověk někdy může špatně interpretovat chování druhé osoby. Například pokud
někdo skáče do postele či jinak prudce lehá, může to vypadat jako nebezpečná
situace. Stejně se v počítačovém vidění nevyhneme falešným poplachům, nicméně
se budeme snažit zapojit různě techniky pro zlepšení přesnosti našeho
detektoru.

Cílem této části práce je vytvořit algoritmus, který pro danou pózu
(reprezentovanou klíčovými body), resp. sekvenci takových póz (získané pro
jednu osobu ze sekvence snímků), určí, zda se jedná o situaci pádu, či nikoliv.
Tento algoritmus bude přijímat vždy pózu jedné osoby, funkcionalita pro více
osob bude řešená v další části práce.

\section{Trénovací data}

Pro trénování našeho modelu jsme použili více než 150 videi. Pro videa byly
vytvořeny anotace aktuální třídy pózy. Tato anotace není pro každý snímek, ale
pouze při změně definuje časovou značku a následující třídu.

Dále byl vytvořen skript, který prošel každé video a vytvořil trénovací data
pro další fázi. Ty obsahují pro každý snímek detekované klíčové body (jako
vstup) a aktuální třídu (jako požadovaný výstup). Pro detekci byl použit dříve
vybraný algoritmus pro detekci pózy. Na použitém algoritmu by teoreticky
nemuselo záležet (pokud detekuje stejné typy klíčových bodů), je ale lepší
použít stejný algoritmus jak pro trénovací data, tak ve výsledném programu.
Algoritmy se totiž můžou v některých situacích chovat trochu jinak (např.
okluze) a náš model by tak dostával v praxi jiná data, než pro jaké byl
natrénován.

Pro trénovací data jsme použili 3 třídy, ty odpovídají třem různým třídám pózy,
které nás zajímají - \textit{normální}, kdy osoba např. chodí, sedí nebo stojí,
\textit{padá} - přechodný stav padání, definován od započatí pohybu směrem
dolů, a \textit{upadl} - definován od momentu, kdy se dotkl zemi trupem nebo
všemi končetinami.

Pro náš model obecně potřebujeme jenom dvě třídy - \textit{normální} a
\textit{upadl}. Nicméně budeme pro náš model experimentovat i s třetí třídou -
\textit{padá}, která pomůže síti hlouběji pochopit problematiku a přesněji
rozeznat některé situace, zejména pak v případě využití rekurentních
neuronových sítí.

\section{Dopředná neuronová síť}
Nejjednodušší architekturou, kterou můžeme pro náš model použít, je dopředná
neuronová síť. Tento model pak bude klasifikovat jednotlivé pózy, aniž by znal
jejich kontext. Síť bude klasifikovat klíčové body pouze podle aktuální
lokalizace ve snímku, nikoliv podle pohybu.

Výhodou této architektury je jednoduchost, potažmo rychlost. Síť nepotřebuje
mnoho parametrů a oproti rekurentním sítím, potřebuje pro evaluaci pouze jeden
dopředný průchod vrstvami sítě.

Další výhodou je jednoduchost trénování a používání. V případě více osob pro
samotnou klasifikaci pádu není nutné sledování osob. Můžeme jednoduše
klasifikovat všechny detekované pózy, aniž bychom řešili, které osobě patří.

Tato síť ale ve výsledku bude klasifikovat pózy pouze dle vzájemného umístění
jednotlivých klíčových bodů, potažmo délky končetin, nebude ale brát v úvahu
natočení postavy. To proto, že postavy ve snímku vystupují pod různým úhlem
natočení v závislosti na natočení kamery. Naopak síť, která je schopná sledovat
pohyb, bude schopna sledovat mj. i změnu natočení postavy a to bez ohledu na
natočení kamery.

\section{Rekurentní neuronové sítě}

Rekurentní neuronové sítě (ang. Recurrent Neural Networks - RNN) je kategorie
neuronových sítí, které do své architektury zapojují zpětnou vazbu. Na rozdíl
od dopředných sítí, které spracovávájí jednotlivé vstupy nezávisle, rekurentní
sítě spolu s aktuálním vstupem při evaluaci zohledňují nějakým způsobem i
výsledek předchozí iterace. Jejich využití tedy je ve dvou oblastech: analýza
změn pozorovaného objektu v čase (např. sledování pohybu, analýza chování či
predikce časových řad) a zpracování kontextuálních informací jako je např.
přirozený jazyk.

Data, která v aktuální iteraci přebíráme z předchozí iterace, se často označují
jako skrytý stav (ang. hidden state). Je to forma paměti, která se s každou
iterací aktualizuje. Často je reprezentován jako stavová vrstva (ang. state
layer nebo context layer), která přijímá hodnoty z výstupu neuronů, uchovává je
mezi iteracemi a předává je spolu se vstupními daty na vstup neuronů. Na
obrázku \ref{fig:rnn} je tato vrstva reprezentována neurony $c_i$.

\subsection{Jednoduché rekurentní sítě}

Nejjednodušší forma rekurentní neuronové sítě je NN s jednou skrytou vrstvou;
tato vrstva kromě dat ze vstupní vrstvy, přijímá také výstup předchozí iterace
buď svých vlastních neuronů, anebo z neuronů výstupní vrstvy, viz obrázek
\ref{fig:rnn}. Obrázek je zjednodušený, v praxi jsou stavová a skrytá vrstva
plně propojeny. Tyto architektury se jmenují Elmanova síť \cite{elman}, resp.
Jordanova síť \cite{jordan}, od jejích tvůrců. Tyto sítě jsou taky známé jako
jednoduché rekurentní sítě (ang. Simple Recurrent Networks - SRN). I když pojem
rekurentních sítí byl známy už od začátků neuronových sítí jako takových a byly
i případy jejích použití, právě tyto sítě patřily k prvním, které používaly pro
trénování algoritmus backpropagation. Jednoduché rekurentní sítě uchovávají
pouze krátkodobé vzory a jsou vhodné spíše pro jednoduché úlohy, jako je např.
predikce časových řad.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/rnn.png}
    \caption{Základní architektury RNN \cite{aksoy}}
    \label{fig:rnn}
\end{figure}

\subsection{Hluboké rekurentní sítě}

Stejně jako u dopředných neuronových sítí, kde se od jednoduchého perceptronu
přešlo k hlubokým sítím, se i rekurentní sítě rozšířily na více vrstev. V
hlubokých rekurentních neuronových sítích (ang. deep RNN - DRNN) jsou pak
jednotlivé vrstvy většinou podobné struktuře Elmanovy sítě - zpětná vazba je
předávána pouze v rámci jedné vrstvy, nikoliv mezi vrstvami RNN (například z
výstupní vrstvy do první skryté vrstvy). Má to několik důvodů. Trénování sítě
ze zpětnou vazbou mezi vrstvami by bylo velmi složité a obtížné. Taky, obecně
každá vrstva sítě se učí pochopit problém na jiné úrovní abstrakce, zpětná
vazba přes několik vrstev by pak mohla narušit stabilitu tohoto procesu a
omezit kvalitu učení.

\subsection{Trénování rekurentních sítí}

Pro pochopení rekurentních neuronových sítí je třeba si vysvětlit, jak se
trénují. Pro vizualizaci trénování RNN se tyto sítě takzvaně rozbaluje v čase
(ang. unrolling). Znamená to, že jednotlivé iterace vizualizujeme jako sekvenci
stejných sítí (stejné váhy), které v čase $t$ přijímají vstup $x_t$ a vracejí
výstup $y_t$, viz obrázek \ref{fig:bptt}. Zároveň místo smyček znázorňujících
zpětnou vazbu, přijímá skrytá vrstva v čase $t$ stav $c_{t-1}$ z předchozí
iterace. Takto je propojená mezi iteracemi každá skrytá vrstva (na obrázku
\ref{fig:bptt} vizualizováno propojení přes stavovou vrstvu).

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/BPTT_weight.png}
    \caption{Unrolling hluboké RNN}
    \label{fig:bptt}
\end{figure}

Při trénování se pak používá algoritmus zpětného šíření chyby v čase (ang.
backpropagation through time - BPTT). Algoritmus funguje stejně jako klasický
backpropagation, šíří se ale nejenom vrstvami ale i iteracemi. Unrolling nám
pomáhá backpropagation pochopit, jednotlivé iterace totiž jsou naskládáne jako
vrstvy a celou síť řešíme jako klasickou dopřednou NN.

\subsection{Problémy mizejícího a explodujícího gradientu}

Výše popsané základní rekurentní neuronové sítě, někdy označovány jako vanilla
RNN, trpí několika zásadními problémy. U dopředných sítí jsme zmiňovali problém
mizejícího gradienutu (ang. vanishing gradient), vystupující zejména u hlubších
sítí. Ten se projevuje i u RNN a je zesílený tím, že jsou jednotlivé iterace
naskládané na sebe, podobně jako vrstvy. Zejména pak u delších sekvencí budou
mít dřívější vstupy velmi malý vliv na učení sítě.

U RNN se taky projevuje problém opačný - explodující gradient (ang. exploding
gradient). Ten způsobuje, že v průběhu sekvence se váhy začnou exponenciálně
zvětšovat a dosáhnou tak nepřiměřeně velkých hodnot.

Podívejme se, co přesně tyto problémy způsobuje. Součástí algoritmu
backpropagation je počítání parciální derivace ztrátové funkce podle
jednotlivých váh. V případě BPTT potřebujeme mimo jiné počítat parciální
derivace skrytého stavu mezi jednotlivými iteracemi $\frac{\partial
        h_{t-1}}{\partial h_t}$. Tyto derivace následně opakovaně násobíme při použití
řetězového pravidla. Pokud je tato derivace $\frac{\partial h_{t-1}}{\partial
        h_t}<1$, jeho vynásobení bude mít za následek postupné zmenšování gradientu.
Pokud budeme například mít sekvencí $100$ iteraci, pak i kdyby se gradienty v
každé iteraci zmenšovaly $0,9$ krát, po $100$ iteracích by gradient klesl na
hodnotu $0,9^{100} \approx 2,7 \times 10^{-5}$, což je prakticky nula. Pokud se
naopak bude gradient zvětšovat $1,1$ krát, po $100$ iteracích by gradient
vzrostl na $1,1^{100} \approx 13 780$, což způsobí úplnou destabilizaci sítě a
nedosáhneme žádného výsledku. Vidíme tedy, že v případě, kdy je $\frac{\partial
        h_{t-1}}{\partial h_t}>1$, dochází k explodujícímu gradientu.

Z důvodu těchto problémů byly vyvinuté složitější rekurentní struktury. Jejich
architektura je v podstatě podobná, jednotlivé vrstvy jsou ale zastoupeny
jinými stavebními bloky, které umožňují zejména širší pochopení kontextu a
efektivnější proces trénování. Vanilla RNN se v praxi dnes využívají velmi
zřídka.

\section{LSTM}

Dlouhá krátkodobá paměť (ang. long short-term memory - LSTM ), představena
Hochreiterem a Schmidhuberem v 1997 roce, je typ rekurentní neuronové sítě,
který byl navržen tak, aby překonal problémy mizejícího a explodujícího
gradientu.

\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/LSTM_unit.pdf}
    \includegraphics[width=0.49\textwidth]{Figures/LSTM_deep.pdf}
    \caption{(1) Jednotka LSTM, (2) Rozvinutá hluboká LSTM síť}
    \label{fig:lstm_unit}
\end{figure}

Její základem je jednotka, viz obrázek \ref{fig:lstm_unit}.1, která ve třech
stádiích aktualizuje krátkodobou a dlouhodobou paměť. Dlouhodobá paměť je
reprezentovaná pomocí cell state (stav buňky), který je postupně upravován a
nakonec předán další iteraci. Dokáže uchovávat dlouhodobé závislosti. Na
obrázku \ref{fig:lstm_unit} je reprezentována proměnnou $c_t$. Krátkodobá paměť
je reprezentována pomocí skrytého stavu. Je použita pro úpravu dlouhodobé
paměti, v konečném stadiu je ale vždy v rámci dané iterace vytvořena nová. Je
tak vhodná pro uchování krátkodobých závislostí. Na obrázku \ref{fig:lstm_unit}
je znázorněna jako $h_t$.

Jednotka LSTM má tři hlavní komponenty - zapomínací bránu (ang. forget gate),
vstupní bránu (ang. input gate) a výstupní bránu (ang. output gate). První,
zapomínací brána určuje, které informace z dlouhodobé paměti se zapomene, resp.
zapamatuje. Na základě neuronu s sigmoidní aktivační funkcí závislém na
aktuálním vstupu, dlouhodobé a krátkodobé paměti vynásobí dlouhodobou paměť, a
tím se určí, které informace budou uchovány nebo zapomenuty. Ve vstupní bráně
se vytvoří pomocí dvou neuronů, se sigmoidní a tang aktivační funkci, nový
vstup do dlouhodobé paměti. Ten je přičten do dlouhodobé paměti. Ve výstupní
bráně se opět pomocí dvou neuronů na základě vstupu, krátkodobé a dlouhodobé
paměti vytvoří novou krátkodobou paměť, která zároveň tvoří výstup z dané
jednotky. Všechny vstupy do neuronů jsou vždy vynásobeny váhami, ty ale nejsou
pro jednoduchost na obrázku \ref{fig:lstm_unit}.1 zobrazeny.

Na obrázku \ref{fig:lstm_unit}.2 je znázorněna rozvinutá hluboká LSTM síť.
Jednotlivé vrstvy sítě jsou naskládány vertikálně, jednotlivé iterace pak jsou
rozvinuty vedle sebe. Jednotlivé vrstvy si předávají skrytý stav - krátkodobou
paměť, mezi iteracemi si pak daná vrstva předává krátkodobou i dlouhodobou
paměť.

LSTM sítě vynikají v udržování dlouhodobých závislostí a složitých struktur.
Jelikož mají tři brány, je síť schopná přesně rozhodnout, které informace chce
dlouhodobě uchovávat, které naopak mají větší vliv na aktuální výstup a které
mají být zapomenuty. Je to ale za cenu většího výpočetního nároku a
složitějšího trénování. Taky je pro tyto sítě vhodné mít větší množství
trénovacích dat, jinak může dojít k přetrénování. 


\endinput