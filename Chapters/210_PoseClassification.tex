\chapter{Klasifikace pózy}
\label{sec:PoseClassification}

Problematika vyhodnocování je velmi široká a přináší mnoho problémů. Ostatně i
člověk někdy může špatně interpretovat chování druhé osoby. Například pokud
někdo skáče do postele či jinak prudce lehá, může to vypadat jako nebezpečná
situace. Stejně se v počítačovém vidění nevyhneme falešným poplachům, nicméně
se budeme snažit zapojit různé techniky pro zlepšení přesnosti našeho
detektoru.

Cílem této části práce je vytvořit algoritmus, který pro danou pózu
(reprezentovanou klíčovými body), resp. sekvenci takových póz (získané pro
jednu osobu ze sekvence snímků), určí, zda se jedná o situaci pádu, či nikoliv.
Tento algoritmus bude přijímat vždy pózu jedné osoby, funkcionalita pro více
osob bude řešená v další části práce.

\section{Trénovací data}

Pro trénování našeho modelu jsme použili více než 150 videí. Pro videa byly
vytvořeny anotace aktuální třídy pózy. Tato anotace není pro každý snímek, ale
pouze při změně definuje časovou značku a následující třídu.

Dále byl vytvořen skript, který prošel každé video a vytvořil trénovací data
pro další fázi. Ty obsahují pro každý snímek detekované klíčové body (jako
vstup) a aktuální třídu (jako požadovaný výstup). Pro detekci byl použit dříve
vybraný algoritmus pro detekci pózy. Na použitém algoritmu by teoreticky
nemuselo záležet (pokud detekuje stejné typy klíčových bodů), je ale lepší
použít stejný algoritmus jak pro trénovací data, tak ve výsledném programu.
Algoritmy se totiž můžou v některých situacích chovat trochu jinak (např.
okluze) a náš model by tak dostával v praxi jiná data, než pro jaké byl
natrénován.

Pro trénovací data jsme použili 3 třídy, ty odpovídají třem různým třídám pózy,
které nás zajímají - \textit{normální}, kdy osoba např. chodí, sedí nebo stojí,
\textit{padá} - přechodný stav padání, definován od započatí pohybu směrem
dolů, a \textit{upadl} - definován od momentu, kdy se dotkl země trupem nebo
všemi končetinami.

Pro náš model obecně potřebujeme jenom dvě třídy - \textit{normální} a
\textit{upadl}. Nicméně budeme pro náš model experimentovat i s třetí třídou -
\textit{padá}, která pomůže síti hlouběji pochopit problematiku a přesněji
rozeznat některé situace, zejména pak v případě využití rekurentních
neuronových sítí.

\section{Dopředná neuronová síť}
Nejjednodušší architekturou, kterou můžeme pro náš model použít, je dopředná
neuronová síť. Tento model pak bude klasifikovat jednotlivé pózy, aniž by znal
jejich kontext. Síť bude klasifikovat klíčové body pouze podle aktuální
lokalizace ve snímku, nikoliv podle pohybu.

Výhodou této architektury je jednoduchost, potažmo rychlost. Síť nepotřebuje
mnoho parametrů a oproti rekurentním sítím potřebuje pro evaluaci pouze jeden
dopředný průchod vrstvami sítě.

Další výhodou je jednoduchost trénování a používání. V případě více osob pro
samotnou klasifikaci pádu není nutné sledování osob. Můžeme jednoduše
klasifikovat všechny detekované pózy, aniž bychom řešili, které osobě patří.

Tato síť ale ve výsledku bude klasifikovat pózy pouze dle vzájemného umístění
jednotlivých klíčových bodů, potažmo délky končetin, nebude ale brát v úvahu
natočení postavy. To proto, že postavy ve snímku vystupují pod různým úhlem
natočení v závislosti na natočení kamery. Naopak síť, která je schopná sledovat
pohyb, bude schopna sledovat mj. i změnu natočení postavy a to bez ohledu na
natočení kamery.

\section{Rekurentní neuronové sítě}

Rekurentní neuronové sítě (ang. Recurrent Neural Networks - RNN) je kategorie
neuronových sítí, které do své architektury zapojují zpětnou vazbu. Na rozdíl
od dopředných sítí, které spracovávají jednotlivé vstupy nezávisle, rekurentní
sítě spolu s aktuálním vstupem při evaluaci zohledňují nějakým způsobem i
výsledek předchozí iterace. Jejich využití tedy je ve dvou oblastech: analýza
změn pozorovaného objektu v čase (např. sledování pohybu, analýza chování či
predikce časových řad) a zpracování kontextuálních informací jako je např.
přirozený jazyk.

Data, která v aktuální iteraci přebíráme z předchozí iterace, se často označují
jako skrytý stav (ang. hidden state). Je to forma paměti, která se s každou
iterací aktualizuje. Často je reprezentován jako stavová vrstva (ang. state
layer nebo context layer), která přijímá hodnoty z výstupu neuronů, uchovává je
mezi iteracemi a předává je spolu se vstupními daty na vstup neuronů. Na
obrázku \ref{fig:rnn} je tato vrstva reprezentována neurony $c_i$.

\subsection{Jednoduché rekurentní sítě}

Nejjednodušší forma rekurentní neuronové sítě je NN s jednou skrytou vrstvou;
tato vrstva kromě dat ze vstupní vrstvy, přijímá také výstup předchozí iterace
buď svých vlastních neuronů, anebo z neuronů výstupní vrstvy, viz obrázek
\ref{fig:rnn}. Obrázek je zjednodušený, v praxi jsou stavová a skrytá vrstva
plně propojeny. Tyto architektury se jmenují Elmanova síť \cite{elman}, resp.
Jordanova síť \cite{jordan}, od jejích tvůrců. Tyto sítě jsou taky známé jako
jednoduché rekurentní sítě (ang. Simple Recurrent Networks - SRN). I když pojem
rekurentních sítí byl známý už od začátků neuronových sítí jako takových a byly
i případy jejích použití, právě tyto sítě patřily k prvním, které používaly pro
trénování algoritmus backpropagation. Jednoduché rekurentní sítě uchovávají
pouze krátkodobé vzory a jsou vhodné spíše pro jednoduché úlohy, jako je např.
predikce časových řad.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/rnn.png}
    \caption{Základní architektury RNN \cite{aksoy}}
    \label{fig:rnn}
\end{figure}

\subsection{Hluboké rekurentní sítě}

Stejně jako u dopředných neuronových sítí, kde se od jednoduchého perceptronu
přešlo k hlubokým sítím, se i rekurentní sítě rozšířily na více vrstev. V
hlubokých rekurentních neuronových sítích (ang. deep RNN - DRNN) jsou pak
jednotlivé vrstvy většinou podobné struktuře Elmanovy sítě - zpětná vazba je
předávána pouze v rámci jedné vrstvy, nikoliv mezi vrstvami RNN (například z
výstupní vrstvy do první skryté vrstvy). Má to několik důvodů. Trénování sítě
ze zpětnou vazbou mezi vrstvami by bylo velmi složité a obtížné. Taky, obecně
každá vrstva sítě se učí pochopit problém na jiné úrovni abstrakce, zpětná
vazba přes několik vrstev by pak mohla narušit stabilitu tohoto procesu a
omezit kvalitu učení.

\subsection{Trénování rekurentních sítí}

Pro pochopení rekurentních neuronových sítí je třeba si vysvětlit, jak se
trénují. Pro vizualizaci trénování RNN se tyto sítě takzvaně rozbaluje v čase
(ang. unrolling). Znamená to, že jednotlivé iterace vizualizujeme jako sekvenci
stejných sítí (stejné váhy), které v čase $t$ přijímají vstup $x_t$ a vracejí
výstup $y_t$, viz obrázek \ref{fig:bptt}. Zároveň místo smyček znázorňujících
zpětnou vazbu přijímá skrytá vrstva v čase $t$ stav $c_{t-1}$ z předchozí
iterace. Takto je propojená mezi iteracemi každá skrytá vrstva (na obrázku
\ref{fig:bptt} vizualizováno propojení přes stavovou vrstvu).

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/BPTT.pdf}
    \caption{Unrolling hluboké RNN}
    \label{fig:bptt}
\end{figure}

Při trénování se pak používá algoritmus zpětného šíření chyby v čase (ang.
backpropagation through time - BPTT). Algoritmus funguje stejně jako klasický
backpropagation, šíří se ale nejenom vrstvami, ale i iteracemi. Unrolling nám
pomáhá backpropagation pochopit, jednotlivé iterace totiž jsou naskládány jako
vrstvy a celou síť řešíme jako klasickou dopřednou NN.

\subsection{Problémy mizejícího a explodujícího gradientu}

Výše popsané základní rekurentní neuronové sítě, někdy označovány jako vanilla
RNN, trpí několika zásadními problémy. U dopředných sítí jsme zmiňovali problém
mizejícího gradientu (ang. vanishing gradient), vystupující zejména u hlubších
sítí. Ten se projevuje i u RNN a je zesílený tím, že jsou jednotlivé iterace
naskládané na sebe, podobně jako vrstvy. Zejména pak u delších sekvencí budou
mít dřívější vstupy velmi malý vliv na učení sítě.

U RNN se taky projevuje problém opačný - explodující gradient (ang. exploding
gradient). Ten způsobuje, že v průběhu sekvence se váhy začnou exponenciálně
zvětšovat a dosáhnou tak nepřiměřeně velkých hodnot.

Podívejme se, co přesně tyto problémy způsobuje. Součástí algoritmu
backpropagation je počítání parciální derivace ztrátové funkce podle
jednotlivých vah. V případě BPTT potřebujeme mimo jiné počítat parciální
derivace skrytého stavu mezi jednotlivými iteracemi $\frac{\partial
        h_{t-1}}{\partial h_t}$. Tyto derivace následně opakovaně násobíme při použití
řetězového pravidla. Pokud je tato derivace $\frac{\partial h_{t-1}}{\partial
        h_t}<1$, jeho vynásobení bude mít za následek postupné zmenšování gradientu.
Pokud budeme například mít sekvencí $100$ iteraci, pak i kdyby se gradienty v
každé iteraci zmenšovaly $0,9$ krát, po $100$ iteracích by gradient klesl na
hodnotu $0,9^{100} \approx 2,7 \times 10^{-5}$, což je prakticky nula. Pokud se
naopak bude gradient zvětšovat $1,1$ krát, po $100$ iteracích by gradient
vzrostl na $1,1^{100} \approx 13 780$, což způsobí úplnou destabilizaci sítě a
nedosáhneme žádného výsledku. Vidíme tedy, že v případě, kdy je $\frac{\partial
        h_{t-1}}{\partial h_t}>1$, dochází k explodujícímu gradientu.

Z důvodu těchto problémů byly vyvinuty složitější rekurentní struktury. Jejich
architektura je v podstatě podobná, jednotlivé vrstvy jsou ale zastoupeny
jinými stavebními bloky, které umožňují zejména širší pochopení kontextu a
efektivnější proces trénování. Vanilla RNN se v praxi dnes využívají velmi
zřídka.

\section{LSTM}

Dlouhá krátkodobá paměť (ang. long short-term memory - LSTM ), představena
Hochreiterem a Schmidhuberem v roce 1997, je typ rekurentní neuronové sítě,
který byl navržen tak, aby překonal problémy mizejícího a explodujícího
gradientu.

\begin{figure}[]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/LSTM_unit.pdf}
    \caption{(1) Jednotka LSTM, (2) Rozvinutá hluboká LSTM síť}
    \label{fig:lstm}
\end{figure}

Její základem je jednotka, viz obrázek \ref{fig:lstm}.1, která ve třech
stádiích aktualizuje krátkodobou a dlouhodobou paměť. Dlouhodobá paměť je
reprezentovaná pomocí stavu buňky (ang. cell state, na obrázku \ref{fig:lstm}.1
$c_t$), který je postupně upravován a nakonec předán další iteraci. Dokáže
uchovávat dlouhodobé závislosti. Krátkodobá paměť je reprezentována pomocí
skrytého stavu. Je použita pro úpravu dlouhodobé paměti, v konečném stadiu je
ale vždy v rámci dané iterace vytvořena nová. Je tak vhodná pro uchování
krátkodobých závislostí. Na obrázku \ref{fig:lstm} je znázorněna jako $h_t$.

Jednotka LSTM má tři hlavní komponenty - zapomínací bránu (ang. forget gate),
vstupní bránu (ang. input gate) a výstupní bránu (ang. output gate). Brány
určují, které informace mají být předány dál.

První, zapomínací brána určuje, které informace z dlouhodobé paměti $c_{t-1}$
se dostanou dále - co má jednotka zapomenout, resp. zapamatovat. Ve vstupní
bráně se nejprve vytvoří kandidátní stav buňky. Ten je výsledkem neuronové
vrstvy s tangenciální aktivační funkcí, do které vstupuje aktuální vstup $x_t$
a krátkodobá paměť $h_{t-1}$. Pak se určí, které informace z kandidátního stavu
buňky se přičtou do stavu buňky a vznikne tak aktuální stav buňky $c_t$. Ve
výstupní bráně se pomocí tangenciální aktivační funkce vytvoří na základě stavu
buňky $c_t$ kandidátní skrytý stav. Pak se určí, které z těchto informací budou
tvořit nový skrytý stav $h_t$.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/LSTM_deep.pdf}
    \caption{Rozvinutá hluboká LSTM síť}
    \label{fig:lstm_deep}
\end{figure}

V každé bráně tedy máme informace, pro které určujeme, zda je poslat dále či
nikoliv, nazvěme je propouštěný obsah (předchozí stav buňky, kandidátní stav
buňky či kandidátní skrytý stav). Toto určení se provádí vždy pomocí neuronové
vrstvy se sigmoidní aktivační funkcí. Do těchto vrstev vstupuje vždy předchozí
skrytý stav $h_{t-1}$ a aktuální vstup $x_t$. Výstupem je hodnota mezi $0$ a
$1$ pro každou informaci. Pak se tento výsledek vynásobí propouštěným obsahem.
Pokud je výstup této vrstvy $0$, informace se nepředávají dál, pokud je $1$,
informace se předávají dále. Vstupy do všech neuronových vrstev jsou vždy
vynásobeny váhami, ty ale nejsou pro jednoduchost na obrázku \ref{fig:lstm}.1
zobrazeny. Na obrázku \ref{fig:lstm_deep} je znázorněna rozvinutá hluboká LSTM
síť. Jednotlivé vrstvy sítě jsou naskládány vertikálně, jednotlivé iterace pak
jsou rozvinuty vedle sebe. Jednotlivé vrstvy si předávají skrytý stav -
krátkodobou paměť, mezi iteracemi si pak daná vrstva předává krátkodobou i
dlouhodobou paměť.

LSTM sítě vynikají v udržování dlouhodobých závislostí a složitých struktur.
Jelikož mají tři brány, je síť schopná přesně rozhodnout, které informace chce
dlouhodobě uchovávat, které naopak mají větší vliv na aktuální výstup a které
mají být zapomenuty. Je to ale za cenu většího výpočetního nároku a
složitějšího trénování. Taky je pro tyto sítě vhodné mít větší množství
trénovacích dat, jinak může dojít k přetrénování. Využívá se tak zejména pro
predikci dlouhých a komplexních časových sekvencí či zpracování přirozeného
jazyka. Zejména u přirozeného jazyka se LSTM sítě osvědčily jako velmi
efektivní. Potřebujeme totiž, aby si síť pamatovala dlouhé závislosti, zároveň
máme většinou k dispozici obrovské množství vzorků.

\endinput