\chapter{Implementace klasifikační neuronové sítě}
\label{chap:ClassificationImplementation}

Problematika vyhodnocování je velmi široká a přináší mnoho problémů. Ostatně i
člověk někdy může špatně interpretovat chování druhé osoby. Například pokud
někdo skáče do postele či jinak prudce lehá, může to vypadat jako nebezpečná
situace. Stejně se v počítačovém vidění nevyhneme falešným poplachům, nicméně
se budeme snažit zapojit různé techniky pro zlepšení přesnosti našeho
detektoru.

Naším úkolem je nyní vytvořit algoritmus, který pro danou pózu (reprezentovanou
klíčovými body), resp. sekvenci takových póz (získané pro jednu osobu ze
sekvence snímků), určí, zda se jedná o situaci pádu, či nikoliv. Tento
algoritmus bude přijímat vždy pózu jedné osoby, funkcionalita pro více osob
bude řešená později.

V kapitole \ref{chap:Pose} jsme pro detekci klíčových bodů zvolili model
\textit{YOLO pose}. Ten je předtrénovaný na datasetu \textit{COCO}, který pro
lidské pózy definuje 17 klíčových bodů v dvourozměrném prostoru. To udává
velikost vstupu do našeho klasifikačního algoritmu. Navrhneme tedy a
natrénujeme neuronovou síť, jejíž vstupem bude 17 2D klíčových bodů - tedy 34
čísel - a výstupem bude klasifikace třídy pózy - \textit{normální} a
\textit{upadl}.

V této kapitole se zaměříme na implementaci klasifikačního algoritmu pro
analýzu pózy. Nejdříve se podíváme na použité technologie a knihovny, které nám
pomohou s vývojem. Poté rozebereme možné architektury sítě a ukážeme, jak byly
implementovány. Dále se zaměříme na návrh vnitřní struktury u techto
architektur, zkusíme je natrénovat v různých konfiguracích a nakonec vybereme
nejoptimálnější řešení.

\section{Použité technologie}

\subsection{PyTorch}

Celý projekt byl vyvíjen v prostředí skriptovacího jazyka Python. Samotný vývoj
neuronových sítí probíhal ve frameworku PyTorch. PyTorch je open-source
knihovna pro strojové učení, široce používaná například pro počítačové vidění
či zpracování přirozeného jazyka. Jeho hlavními funkcemi je práce s tenzory
podobnými jako v NumPy, ale s podporou silné akcelerace s využitím GPU, a vývoj
neuronových sítí postavený na vysokoúrovňových stavebních blocích s podporou
automatické derivace pro počítaní gradientů.

Implementace neuronových sítí v PyTorch je velice jednoduchá a díky vysoké míře
abstrakce umožňuje se při vývoji soustředit na samotnou architekturu a design
sítě, nikoliv na detaily implementace. Pro vytvoření nové neuronové sítě stačí
vytvořit třídu, která bude dědit od základní třídy $nn.Module$, v konstruktoru
definovat jednotlivé vrstvy včetně různých regularizačních hyperparametrů. V
metodě $forward$ pak definujeme dopředný průchod sítě. Dále máme možnost
kontrolovat např. výpočet ztrátové funkce či metriky přesnosti.

PyTorch obsahuje také předpřipravené moduly pro GRU či LSTM sítě, včetně
vícevrstvých architektur. Pro tyto moduly definujeme velikost vstupu a velikost
skrytého stavu, dále můžeme definovat počet vrstev či dropout. Výstup z těchto
modulů je pak skrytý stav, který můžeme dále zpracovávat pomocí jednoduché
dopředné sítě pro predikci třídy.

\subsection{Lightning}
\label{sec:Lightning}

PyTorch Lightning je wrapper pro PyTorch, který dále usnadňuje vývoj
neuronových sítí. Stará se za nás o detaily procesu trénování, jako je správa
epoch, logování či optimalizace kroku učení (ang. learning rate). Podporuje
taky nativně práci s TensorBoard, což je logovací nástroj umožňující přehledné
sledování metrik během trénování, včetně grafického zobrazení ve webovém
prostředí.

Pro implementaci modelu vytvoříme třídu, která dědí z
$lightning.LightningModule$, a kromě konstruktoru, ve kterém inicializujeme
jednotlivé vrstvy, definujeme metody $training\_step$ (krok trénování),
$validation\_step$ - krok validace, $configure\_optimizers$ - definování
optimalizační techniky a $forward$, která definuje, jak signál prochází
jednotlivými vrstvami.

\section{Implementace vybraných architektur}
\label{sec:SelectedArchitectures}

Podíváme se nyní, jak jsme jednotlivé architektury implementovali, nezávisle na
jejich vnitřní struktuře a konfiguraci.

\subsection{Dopředná neuronová síť}

Nejjednodušší architekturou, kterou můžeme pro náš model použít, je dopředná
neuronová síť. Tento model pak bude klasifikovat jednotlivé pózy, aniž by znal
jejich kontext. Síť bude klasifikovat klíčové body pouze podle aktuální
lokalizace ve snímku, nikoliv podle pohybu.

Výhodou této architektury je jednoduchost, potažmo rychlost. Síť nepotřebuje
mnoho parametrů a oproti rekurentním sítím potřebuje pro evaluaci pouze jeden
dopředný průchod vrstvami sítě.

Další výhodou je jednoduchost trénování a používání. V případě více osob pro
samotnou klasifikaci pádu není nutné sledování osob. Můžeme jednoduše
klasifikovat všechny detekované pózy, aniž bychom řešili, které osobě patří.

Tato síť ale ve výsledku bude klasifikovat pózy pouze dle vzájemného umístění
jednotlivých klíčových bodů, potažmo délky končetin, nebude ale brát v úvahu
natočení postavy. To proto, že postavy ve snímku vystupují pod různým úhlem
natočení v závislosti na natočení kamery. Naopak síť, která je schopná sledovat
pohyb, bude schopna sledovat mj. i změnu natočení postavy a to bez ohledu na
natočení kamery.

Při použití této architektury jsou jako vstupní data použity klíčové body jedné
osoby z daného jednoho snímku. Trénovací data pak pouze přečteme z trénovacího
souboru a překonvertujeme je do formy tenzoru, konkrétně je zabalíme do
instance třídy $DataLoader$. Při použití ve výsledném programu předáme modelu
klíčové body každé detekované osoby v daném snímku.

Výstupem sítě je hodnota od $0$ do $1$, čísla od $0$ do $0.5$ jsou považována
za třídu \textit{normální}, zatímco čísla od $0.5$ do $1$ za třídu
\textit{upadl}.

Pro implementaci dopředné sítě v Pytorch Lightning jsme použili modul
$nn.sequential$, ve kterém definujeme postupné kroky průchodů sítě. $nn.Linear$
definuje vrstvu sítě včetně počtu neuronů předcházející a aktuální vrstvy. Dále
můžeme definovat aktivační funkci po dané vrstvě a regularizační techniky jako
je dropout či normalizace. Příklad implementace dopředné sítě je uveden v kódu
\ref{src:ffnn}. V tomto příkladě jsou definovány dvě vnitřní vrstvy o velikosti
$128$ a $32$ a výstupní vrstva s jedním neuronem. Mezi vrstvami je aplikována
normalizace a dropout $0.3$, jako aktivační funkce je použita ReLU.

\begin{lstlisting}[language=Python, label=src:ffnn, caption={Ukázka implementace Dopředné sítě v PyTorch Lightning}]
class KeypointClassifierFFNN(L.LightningModule):
    def __init__(self):
        super(KeypointClassifierFFNN, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(34, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
        )
        self.criterion = nn.BCEWithLogitsLoss()
        self.accuracy = BinaryAccuracy(threshold=0.5)
        self.sigmoid = nn.Sigmoid()    
    def forward(self, x):
        x = self.classifier(x)
        return x
    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-4)
        return optimizer
    def training_step(self, batch, batch_idx):
        input, target = batch
        output = self(input)
        loss = self.criterion(output, target.float())    
        self.log("train_loss", loss, on_epoch=True, on_step=False)
        return loss
    def validation_step(self, batch, batch_idx):
        data, target = batch
        output = self(data)
        loss = self.criterion(output, target.float())
        self.log('val_loss', loss, on_epoch=True, on_step=False)
        output = self.sigmoid(output)
        accuracy = self.accuracy(output, target.int())    
        self.log('val_accuracy', accuracy, on_epoch=True, on_step=False)
    return loss
\end{lstlisting}

Abychom mohli tuto síť efektivně trénovat, musíme umožnit jednoduchou změnu
konfigurace sítě. Třída $KeypointClassifierFFNN$ proto v konstruktoru přijímá
parametry \textit{layers} - pole čísel reprezentujících velikosti jednotlivých
vrstev, \textit{activation} - identifikátor vybrané aktivační funkce a
\textit{dropout} definující velikost dropoutu, viz kód \ref{src:ffnn_params}.
Jako ztrátovou funkci jsme použili křížovou entropii, tedy
$nn.BCEWithLogitsLoss()$, která je vhodná pro binární klasifikaci.

\begin{lstlisting}[language=Python, label=src:ffnn_params, caption={Parametrizace konfigurace dopředné sítě}] 
class KeypointClassifierFFNN(L.LightningModule):
    def __init__(self, layers, activation, dropout=0.3, device=None):
        super(KeypointClassifierFFNN, self).__init__()        
        classifier = nn.Sequential()
        for i in range(len(layers)-1):
            classifier.add_module(f'layer_{i}', nn.Linear(layers[i], layers[i+1]))
            classifier.add_module(f'batch_norm_{i}', nn.BatchNorm1d(layers[i+1]))
            classifier.add_module(f'activation_{i}', activations[activation])
            classifier.add_module(f'dropout_{i}', nn.Dropout(dropout))
        classifier.add_module(f'layer_{len(layers)-1}', nn.Linear(layers[-1], 1))
        self.classifier = classifier
        self.criterion = nn.BCEWithLogitsLoss()
        self.accuracy = BinaryAccuracy(threshold=0.5)
        self.layers = layers
        self.sigmoid = nn.Sigmoid()   
\end{lstlisting}

\subsection{GRU síť}

Jelikož pád je událost, nikoliv statická póza, mohlo by být optimálnější použít
algoritmus, který bude analyzovat nejenom aktuální pózu, ale sekvenci
posledních $n$ póz. Pro tento účel se nabízí rekurentní neuronové sítě. V
dnešní době je nejpoužívanější rekurentní architekturou GRU (Gated Recurrent
Unit).

Příklad implementace GRU sítě je uveden v kódu \ref{src:gru}, kde je definována
GRU jednotka s jednou vrstvou, velikosti vstupního vektoru $34$ a skrytým
stavem velikosti $128$, následována dvouvrstvou dopřednou plně propojenou sítí
ze $128$ neurony v první vrstvě a s jedním neuronem ve druhé vrstvě.

\begin{lstlisting}[language=Python, label=src:gru, caption={Ukázka implementace GRU sítě v PyTorch Lightning}]
    class KeypointClassifierGRU(lightning.LightningModule):
        def __init__(self):
            super(KeypointClassifierGRU, self).__init__()
            self.gru = nn.GRU(34, 128)              # GRU vrstvy
            self.classifier = nn.Sequential(        # Plně propojené vrstvy
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1) )
            self.criterion = nn.BCEWithLogitsLoss ()    # Ztrátová funkce
            self.accuracy = BinaryAccuracy()            # Metrika přesnosti
        def forward(self, x):
            _, x = self.gru(x)
            x = x[-1]                               # Poslední skrytý stav
            x = self.classifier(x)
            return x
    
\end{lstlisting}

Část těchto hyperparametrů jsme opět parametrizovali, abychom mohli postupně
otestovat různé jejich kombinace a vyhodnotit jejich vliv na výkon modelu, viz
kód \ref{src:params}. Jedná se o velikost vstupního vektoru, velikost skrytého
stavu sítě GRU, počet vrstev GRU, velikost první plně propojené vrstvy,
velikost výstupní vrstvy (pro binární klasifikaci vždy $1$, potřebné pro
pozdější optimalizace) a dropout pro GRU i plně propojené vrstvy. Všechny tyto
hyperparametry budeme ladit v další části.

\begin{lstlisting}[language=Python, label=src:params, caption={Parametry konstruktoru třídy $KeypointClassifierGRU$ definující hyperparametry sítě}]
class KeypointClassifierGRU(L.LightningModule):
    def __init__(self, input_size=34, rnn_hidden_size=128, rnn_layers_count=2, fc_size=128, output_size=1, rnn_dropout=0.3, fc_dropout=0.3, device=None):
\end{lstlisting}

Stejně jako u předchozí architektury, jako ztrátová funkce byla použita binární
křížová entropie, tedy $nn.BCEWithLogitsLoss()$, která je vhodná pro binární
klasifikaci.

\subsection{LSTM síť}

Další populární rekurentní architekturou je LSTM (Long Short-Term Memory). Ta
je lepší pro delší sekvence, je to ale za cenu větší komplexity. Jak již bylo
vysvětleno, s nárůstem komplexity se zvyšuje riziko přetrénování, zejména při
nedostatku trénovacích dat. Vzhledem k množství trénovacích dat, které máme k
dispozici, se tak dá předpokládat, že trénování této sítě bude spíše méně
stabilní než v případě sítě GRU.

Implementace sítě LSTM se od GRU liší pouze použitím modulu $nn.LSTM$ místo
$nn.GRU$. Použití těchto modulů je v PyTorch velice podobné, pouze LSTM vrací
kromě skrytého stavu také stav buňky (dlouhodobou paměť), ten ale stejně
nevyužijeme. Pro síť LSTM budeme ladit stejnou množinu hyperparametrů, jako pro
GRU síť, abychom mohli porovnat jejich výkonnost.

\section{Návrh architektury a konfigurace sítě}

Nyní přistoupíme do samotného návrhu architektury a konfigurace sítě. Vybrané
architektury jsme natrénovali v různých konfiguracích a celý postup trénování
jsme zapisovali pomocí logovacího nástroje TensorBoard. Nyní na základě grafů
základních metrik, jako jsou ztrátová funkce či přesnost, zhodnotíme výkon
vybraných architektur, podíváme se, jaký mají jednotlivé hyperparametry vliv na
proces trénování a výsledný výkon. Nakonec vybereme nejoptimálnější variantu.

K hyperparametrům, které jsme zkoušeli, patří počet vrstev a jejich velikost a
velikost dropoutu. U dopředných sítí jsme navíc zkoušeli i různé aktivační
funkce, u rekurentních sítí pak velikost skrytého stavu. Pro optimalizaci váh
během trénování byl použit optimalizační algoritmus Adam \cite{adam}, jehož
výhodou je

\subsection{Dopředná neuronová síť}

Nejprve jsme zkoušeli natrénovat dopřednou neuronovou síť. Vyzkoušeli jsme od
dvou do čtyř vnitřních vrstev, ve velikostech od $32$ do $512$ neuronů, vždy
mocniny dvou. Pojďme postupně rozebrat jednotlivé hyperparametry a jejich vliv
na výkon modelu.

U dopředných sítí se nám nejlépe ověřila velikost dávky $4096$. Síť se lépe
trénovala s použitím normalizace (modul $nn.BatchNorm1d$) a dropoutu. Pro
většinu konfigurací byla velikost dropoutu $0.4$ nejoptimálnější.

Síť jsme zkoušeli trénovat s těmito vybranými aktivačními funkcemi: $ReLU$,
$Tanh$, $PReLU$ a $Mish$. $ReLu$ a $PReLu$ často dosahovaly podobných výsledků,
většinou ale nejlepších výsledků dosahovala $ReLU$. Příkladem je graf přesnosti
pro trénování sítě se třemi vnitřními vrstvami velikosti $128$, $64$, a $32$ na
obrázku \ref{graph:fnnactivations}.

\begin{figure}[] % 'htbp' controls figure placement (here, top, bottom, page)
    \centering
    \caption{Graf přesnosti na validačních datech v průbehu trénování pro různé použité aktivační funkce}
    \label{graph:fnnactivations}
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={Epochy},
                ylabel={Validační ztráta},
                ymin=0.7,
                grid=both,
                width=0.9\textwidth,
                height=0.4\textheight,
                legend pos=south east,
            ]
            \addplot[
                color=blue,
                mark=*,
                no markers
            ]
            table {Plots/relu.dat};
            \addlegendentry{ReLu}
            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/prelu.dat};
            \addlegendentry{PReLu}
            \addplot[
                color=orange,
                mark=*,
                no markers
            ]
            table {Plots/tanh.dat};
            \addlegendentry{Tanh}
            \addplot[
                color=purple,
                mark=*,
                no markers
            ]
            table {Plots/mish.dat};
            \addlegendentry{Mish}
        \end{axis}
    \end{tikzpicture}
\end{figure}

Obecně větší sítě dosahovaly lepších výsledků - menší ztrátové funkce a vyšší
přesnosti na validačních datech, v jistém momentě jsme již ale začali narážet
na problém se stabilitou trénování. Nejoptimálnějších výsledků dosáhla síť se
třemi vrstvami o velikostech $128$, $64$ a $32$. Jak můžeme vidět na grafu
\ref{graph:deepffnn}, sítě s větší šířkou nebo hloubkou sice dosahují lepších
výsledků, začínají ale brzy výrazně kolísat. Zkoušeli jsme v těchto případech i
L2 regularizaci (parametr $weight\_decay$), to ale neřešilo problém. Pro
komplexnější sítě bychom pravděpodobně potřebovali větší množství dat.

Pojďme si tedy shrnout, jaká je pro nás nejoptimálnější konfigurace: použili
jsme tři vrstvy o velikostech $128$, $64$ a $32$, dropout $0.3$, normalizaci
mezi vrstvami, velikost dávky 4096 a aktivační funkci $ReLu$. Dosáhli jsme tak
validační ztráty $0.18$ a validační přesnosti 93.8%

\begin{figure}[] % 'htbp' controls figure placement (here, top, bottom, page)
    \centering
    \caption{Graf validační ztráty v průběhu trénování hlubší sítě }
    \label{graph:deepffnn}
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={Epochy},
                ylabel={Validační ztráta},
                grid=both,
                width=0.9\textwidth,
                height=0.5\textheight,
                legend pos=north east,
                ymax=0.25,
            ]

            \addplot[
                color=blue,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 128, 64, 32}
            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,256,128,32].dat};
            \addlegendentry{Velikosti vrstev: 256, 128, 32}
            \addplot[
                color=orange,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,256,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 256, 128, 64, 32}
            \addplot[
                color=purple,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,512,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 512, 128, 64, 32}
        \end{axis}
    \end{tikzpicture}
\end{figure}

\subsection{GRU síť}

Jak již bylo zmíněno, v oblasti jednodušších RNN jsou dnešním standardem GRU
sítě, sítě LSTM se používají zejména, pokud si GRU s problémem neradí, anebo je
vzhledem k problému důležité uchování dlouhodobých závislostí.

Stejnou taktiku zvolíme i my. Nejdříve otestujeme GRU sítě, a to pro několik
možností délky analyzované sekvence. To znamená, že pro každý snímek předáme
síti očekávanou třídu a sekvenci póz dané osoby z $n$ posledních snímků. Pak
vyzkoušíme síti předávat celou sekvenci póz dané osoby.

Naše GRU síť se skládá z několika vrstev GRU, konkrétně jsme zkoušeli trénovat
1 až 3 vrstvy, které následuje jedná plně propojená vrstva a výstupní vrstva.
Velikosti GRU vrstev jsme volili v rozsahu od 64 do 256, u plně propojené
vrstvy jsme volili mezi 32 a 128 neurony. Většinou jsme používali dávky o
velikosti $4096$, menší dávky velice destabilizovaly trénování.

Zjistili jsme, že na rozdíl od dopředných sítí, v případě GRU nejlepších
výsledků dosahovaly jednodušší sítě. Většina sítí s jednou GRU vrstvou tak
dosahovala poměrně stabilních výsledků. Nejvyšší přesnosti jsme pak dosáhli
sítí s jednou GRU vrstvou o velikosti $64$ a plně propojenou vrstvou o
velikosti $64$, s dropoutem 0.15 v GRU vrstvě a 0.4 v plně propojené vrstvě. I
když většinou bylo dosaženo stabilnějších výsledků s větším dropoutem jako je
0.4, některé jednodušší sítě se lépe trénovaly s menším dropoutem.

Sítě s více GRU vrstvami většinou než dosáhly optimálního výkonu, začaly
kolísat a ve validační ztrátě a přesnosti se objevovaly obrovské výkyvy.
Zkoušeli jsme větší dávky - $8162$, které sice stabilizovaly trénování při
stejném počtu epoch, jelikož se ale vetší dávka loučí s pomalejším učením,
nedosahovaly takové přesnosti jako při dávkách velikosti $4096$. Pro dosažení
podobné přesnosti jsme tedy zkoušeli trénovat ve více epochách, tehdy ale brzy
docházelo k přetrénování.

RNN sítě obecně můžou přijímat sekvence libovolné délky, je ale efektivnější,
pokud je síť trénovaná na jednotné délce, tato délka sekvence je pak použitá i
v případě inferencí. Zkoušeli jsme tedy trénovat sítě na sekvencích o délce
$50$ a $100$ snímků, vyzkoušeli jsme i neomezenou délku sekvence, tedy modelu
předáváme klíčové body z celé sekvence snímků, na kterých byla daná osoba
detekována. Obecně jsme ale zjistili, že vždycky dosahují stabilnějších
výsledků sítě spíše s kratšími sekvencemi. Zůstali jsme tedy u sekvencí délky
$50$.

\subsection{LSTM síť}

LSTM sítě jsme trénovali se stejnými hyperparametry a konfiguracemi jako sítě
GRU. Obecně se taky dá říct, že měly tyto parametry u obou architektur velice
podobný vliv na výsldky, a tedy to co jsme napsali o GRU sítích, by se většinou
dalo napsát i o LSTM sítích. Taky měly nejlepší výsledky s jednou vrstvou,
podobně se projevovala déelka sekvence, a obdobně byl většinou optimální
dropout velikosti $0.4$.

Obecně ale LSTM sítě byly mnohem méně stabilní, a prakticky v žádné konfiguraci
se nepodařilo natrénovat síť do požadované přesnosti, aniž by došlo k
přetrénování anebo by začala validační ztráta extrémně kolísat. Pravděpodobně
tedy nemáme dostatečný vzorek trénovacích dat pro trénování LSTM sítě.


%
%
%
% TODO ::(
%
%
%
%
%
%
%
%
%
%
%
%
%

% \endinput
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% \subsection{Vícetřídní klasifikace}

% Jak již bylo zmíněno (\ref{sec:TrainingData}), pro naše řešení potřebujeme
% detekovat pouze třídy \textit{normální} a \textit{upadl}, vyzkoušíme ale
% natrénovat i modely se třemi třídami - přidáme třídu \textit{padá}. Nemá smysl
% v našem případě implementovat takovou dopřednou neuronovou síť, jelikož
% klasifikuje snímky bez kontextu předchozích snímků.

% Budeme tedy navíc implementovat sítě GRU a LSTM, s velice podobnou topologií
% jako sítě s binární klasifikací. Hlavním rozdílem bude velikost výstupní
% vrstvy, jež bude rovna počtu tříd a použitá ztrátová funkce - nyní použijeme
% křížovou entropii. Výstup vícetřídní klasifikační sítě pak představuje pole
% pravděpodobností, že se jedná o danou třídu. Při inferenci pak použijeme
% jednotku softmax pro převod výstupu na číslo třídy.