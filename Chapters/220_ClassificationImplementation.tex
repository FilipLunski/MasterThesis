\chapter{Implementace neuronové sítě pro kalsifikaci klíčových bodů}
\label{chap:ClassificationImplementation}

Problematika vyhodnocování situací ve videu je velmi široká a přináší mnoho
problémů. Ostatně i člověk někdy může špatně interpretovat chování druhé osoby.
Například pokud někdo skáče do postele či jinak prudce lehá, může to vypadat
jako nebezpečná situace. Stejně se v počítačovém vidění nelze vyhnout falešným
poplachům, nicméně budou zapojeny různé techniky za účelem zlepšení přesnosti
detektoru pádu.

Úkolem je nyní vytvořit algoritmus, který pro danou pózu (reprezentovanou
klíčovými body), resp. sekvenci takových póz (získané pro jednu osobu ze
sekvence snímků), určí, zda se jedná o situaci pádu, či nikoliv. Tento
algoritmus bude přijímat vždy pózu jedné osoby, funkcionalita pro více osob
bude řešená později.

V kapitole \ref{chap:Pose} byl pro detekci klíčových bodů zvolen model
\textit{YOLO pose}. Ten je předtrénovaný na datasetu \textit{COCO}, který pro
lidské pózy definuje 17 klíčových bodů v dvourozměrném prostoru. To udává
velikost vstupu do našeho klasifikačního algoritmu. Bude tedy navrhnuta a
natrénována neuronová síť, jejíž vstupem bude 17 2D klíčových bodů – tedy 34
čísel – a výstupem bude klasifikace třídy pózy – \textit{normální} a
\textit{upadl}.

Tato kapitola bude zaměřena na implementaci klasifikačního algoritmu pro
analýzu pózy. Nejdříve budou krátce popsány použité technologie a knihovny,
které pomohou při vývoji. Poté budou rozebrány možné architektury sítě a bude
ukázáno, jak byly implementovány nezávisle na výsledné struktuře. Dále bude
navržena vnitřní struktura těchto architektur, a to tak, že budou trénovány v
různých konfiguracích a bude vybráno nejoptimálnější řešení. Nakonec budou
výsledné modely otestovány na testovací sadě.

\section{Použité technologie}

\subsection{PyTorch}

Celý projekt byl vyvíjen v prostředí skriptovacího jazyka Python. Samotný vývoj
neuronových sítí probíhal ve frameworku PyTorch. PyTorch je open-source
knihovna pro strojové učení, široce používaná například pro počítačové vidění
či zpracování přirozeného jazyka. Jeho hlavními funkcemi je práce s tenzory
podobnými jako v NumPy, ale s podporou silné akcelerace s využitím GPU, a vývoj
neuronových sítí postavený na vysokoúrovňových stavebních blocích s podporou
automatické derivace pro počítaní gradientů.

Implementace neuronových sítí v PyTorch je velice jednoduchá a díky vysoké míře
abstrakce umožňuje soustředit se při vývoji na samotnou architekturu a design
sítě, nikoliv na detaily implementace. Pro vytvoření nové neuronové sítě stačí
vytvořit třídu, která bude dědit od základní třídy $nn.Module$, v konstruktoru
definovat jednotlivé vrstvy včetně různých regularizačních hyperparametrů. V
metodě $forward$ pak definujeme dopředný průchod sítě. Dále máme možnost
kontrolovat např. výpočet ztrátové funkce či metriky přesnosti.
%todo cite docs

PyTorch obsahuje také předpřipravené moduly pro GRU či LSTM sítě, včetně
vícevrstvých architektur. Pro tyto moduly se definuje velikost vstupu a
velikost skrytého stavu, dále lze definovat počet vrstev či dropout. Výstup z
těchto modulů je pak skrytý stav, který je dále možné zpracovávat pomocí
jednoduché dopředné sítě pro predikci třídy.

\subsection{Lightning}
\label{sec:Lightning}

PyTorch Lightning je wrapper pro PyTorch, který dále usnadňuje vývoj
neuronových sítí. Stará se o detaily procesu trénování, jako je správa epoch,
logování či optimalizace kroku učení (ang. learning rate). Podporuje také
nativně práci s TensorBoard, což je logovací nástroj umožňující přehledné
sledování metrik během trénování, včetně grafického zobrazení ve webovém
prostředí.

Pro implementaci modelu je třeba vytvořit třídu, která dědí z
$lightning.LightningModule$, a kromě konstruktoru, ve kterém jsou
inicializovány jednotlivé vrstvy, se definují metody $training\_step$ (krok
trénování), $validation\_step$ – krok validace, $configure\_optimizers$ –
definování optimalizační techniky a $forward$, která definuje, jak signál
prochází jednotlivými vrstvami.

\section{Implementace vybraných architektur}
\label{sec:SelectedArchitectures}

Nyní bude popsána implementace jednotlivých architektur, nezávisle na jejich
vnitřní struktuře a konkrétní konfiguraci. Celou konfiguraci bude možné předat
jako argumenty do konstruktoru modelu. Pro návrh struktury tak bude možné
experimentovat z hyperparametry a konfiguracemi, s cílem najít optimální
řešení.

\subsection{Dopředná neuronová síť}

Nejjednodušší architekturou, kterou je možné pro klasifikační model použít, je
dopředná neuronová síť. Tento model pak bude klasifikovat jednotlivé pózy, aniž
by znal jejich kontext. Síť bude klasifikovat klíčové body pouze podle aktuální
lokalizace ve snímku, nikoliv podle pohybu.

Výhodou této architektury je jednoduchost, potažmo rychlost. Síť nepotřebuje
mnoho parametrů a oproti rekurentním sítím potřebuje pro evaluaci pouze jeden
dopředný průchod vrstvami sítě.

Další výhodou je jednoduchost trénování a používání. V případě více osob pro
samotnou klasifikaci pádu není nutné sledování osob. Lze jednoduše klasifikovat
všechny detekované pózy, aniž by se řešilo, které osobě patří.

Tato síť ale ve výsledku bude klasifikovat pózy pouze dle vzájemného umístění
jednotlivých klíčových bodů, potažmo délky končetin, nebude ale brát v úvahu
natočení postavy. To proto, že postavy ve snímku vystupují pod různým úhlem
natočení v závislosti na natočení kamery. Naopak síť, která je schopná sledovat
pohyb, bude schopna sledovat mj. i změnu natočení postavy a to bez ohledu na
natočení kamery.

Při použití této architektury jsou jako vstupní data použity klíčové body jedné
osoby z daného jednoho snímku. Trénovací data jsou pak pouze čteny z
trénovacího souboru a konvertovány do formy tenzoru, konkrétně jsou zabalovány
do instance třídy $DataLoader$. Při použití ve výsledném programu jsou modelu
předávány klíčové body každé detekované osoby v daném snímku.

Výstupem sítě je hodnota od $0$ do $1$, čísla od $0$ do $0.5$ jsou považována
za třídu \textit{normální}, zatímco čísla od $0.5$ do $1$ za třídu
\textit{upadl}.

Pro implementaci dopředné sítě v Pytorch Lightning byl použit modul
$nn.Sequential$, ve kterém se definují postupné kroky průchodů sítě.
$nn.Linear$ definuje vrstvu sítě včetně počtu neuronů předcházející a aktuální
vrstvy. Dále lze definovat aktivační funkci po dané vrstvě a regularizační
techniky jako je dropout či normalizace. Příklad implementace dopředné sítě je
uveden v kódu \ref{src:ffnn}. V tomto příkladě jsou definovány dvě vnitřní
vrstvy o velikosti $128$ a $32$ a výstupní vrstva s jedním neuronem. Mezi
vrstvami je aplikována normalizace a dropout $0.3$, jako aktivační funkce je
použita ReLU.

\begin{lstlisting}[language=Python, label=src:ffnn, caption={Ukázka implementace Dopředné sítě v PyTorch Lightning}]
class KeypointClassifierFFNN(L.LightningModule):
    def __init__(self):
        super(KeypointClassifierFFNN, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(34, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1),
        )
        self.criterion = nn.BCEWithLogitsLoss()
        self.accuracy = BinaryAccuracy(threshold=0.5)
        self.sigmoid = nn.Sigmoid()    
    def forward(self, x):
        x = self.classifier(x)
        return x
    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-4)
        return optimizer
    def training_step(self, batch, batch_idx):
        input, target = batch
        output = self(input)
        loss = self.criterion(output, target.float())    
        self.log("train_loss", loss, on_epoch=True, on_step=False)
        return loss
    def validation_step(self, batch, batch_idx):
        data, target = batch
        output = self(data)
        loss = self.criterion(output, target.float())
        self.log('val_loss', loss, on_epoch=True, on_step=False)
        output = self.sigmoid(output)
        accuracy = self.accuracy(output, target.int())    
        self.log('val_accuracy', accuracy, on_epoch=True, on_step=False)
    return loss
\end{lstlisting}

Aby bylo možné tuto síť efektivně trénovat, je třeba umožnit jednoduchou změnu
konfigurace sítě. Třída $KeypointClassifierFFNN$ proto v konstruktoru přijímá
parametry \textit{layers} – pole čísel reprezentujících velikosti jednotlivých
vrstev, \textit{activation} – identifikátor vybrané aktivační funkce a
\textit{dropout} definující velikost dropoutu, viz kód \ref{src:ffnn_params}.
Jako ztrátová funkce byla použita křížová entropie, tedy
$nn.BCEWithLogitsLoss()$, která je vhodná pro binární klasifikaci.

\begin{lstlisting}[language=Python, label=src:ffnn_params, caption={Parametrizace konfigurace dopředné sítě}] 
class KeypointClassifierFFNN(L.LightningModule):
    def __init__(self, layers, activation, dropout=0.3, device=None):
        super(KeypointClassifierFFNN, self).__init__()        
        classifier = nn.Sequential()
        for i in range(len(layers)-1):
            classifier.add_module(f'layer_{i}', nn.Linear(layers[i], layers[i+1]))
            classifier.add_module(f'batch_norm_{i}', nn.BatchNorm1d(layers[i+1]))
            classifier.add_module(f'activation_{i}', activations[activation])
            classifier.add_module(f'dropout_{i}', nn.Dropout(dropout))
        classifier.add_module(f'layer_{len(layers)-1}', nn.Linear(layers[-1], 1))
        self.classifier = classifier
        self.criterion = nn.BCEWithLogitsLoss()
        self.accuracy = BinaryAccuracy(threshold=0.5)
        self.layers = layers
        self.sigmoid = nn.Sigmoid()   
\end{lstlisting}

\subsection{GRU síť}

Jelikož pád je událost, nikoliv statická póza, mohlo by být optimálnější použít
algoritmus, který bude analyzovat nejenom aktuální pózu, ale sekvenci
posledních $n$ póz. Pro tento účel se nabízí rekurentní neuronové sítě. V
dnešní době je nejpoužívanější rekurentní architekturou GRU (Gated Recurrent
Unit).

Příklad implementace GRU sítě je uveden v kódu \ref{src:gru}, kde je definována
GRU jednotka s jednou vrstvou, velikosti vstupního vektoru $34$ a skrytým
stavem velikosti $128$, následována dvouvrstvou dopřednou plně propojenou sítí
ze $128$ neurony v první vrstvě a s jedním neuronem ve druhé vrstvě.

\begin{lstlisting}[language=Python, label=src:gru, caption={Ukázka implementace GRU sítě v PyTorch Lightning}]
    class KeypointClassifierGRU(lightning.LightningModule):
        def __init__(self):
            super(KeypointClassifierGRU, self).__init__()
            self.gru = nn.GRU(34, 128)              # GRU vrstvy
            self.classifier = nn.Sequential(        # Plně propojené vrstvy
                nn.Linear(128, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1) )
            self.criterion = nn.BCEWithLogitsLoss ()    # Ztrátová funkce
            self.accuracy = BinaryAccuracy()            # Metrika přesnosti
        def forward(self, x):
            _, x = self.gru(x)
            x = x[-1]                               # Poslední skrytý stav
            x = self.classifier(x)
            return x
    
\end{lstlisting}

Část těchto hyperparametrů byla opět parametrizována, aby bylo možné postupně
otestovat různé jejich kombinace a vyhodnotit jejich vliv na výkon modelu, viz
kód \ref{src:params}. Jedná se o velikost vstupního vektoru, velikost skrytého
stavu sítě GRU, počet vrstev GRU, velikost první plně propojené vrstvy,
velikost výstupní vrstvy (pro binární klasifikaci vždy $1$, potřebné pro
pozdější optimalizace) a dropout pro GRU i plně propojené vrstvy. Všechny tyto
hyperparametry se budou ladit v další části.

\begin{lstlisting}[language=Python, label=src:params, caption={Parametry konstruktoru třídy $KeypointClassifierGRU$ definující hyperparametry sítě}]
class KeypointClassifierGRU(L.LightningModule):
    def __init__(self, input_size=34, rnn_hidden_size=128, rnn_layers_count=2, fc_size=128, output_size=1, rnn_dropout=0.3, fc_dropout=0.3, device=None):
\end{lstlisting}

Stejně jako u předchozí architektury, jako ztrátová funkce byla použita binární
křížová entropie, tedy $nn.BCEWithLogitsLoss()$, která je vhodná pro binární
klasifikaci.

\subsection{LSTM síť}

Další populární rekurentní architekturou je LSTM (Long Short-Term Memory). Ta
je lepší pro delší sekvence, je to ale za cenu větší komplexity. Jak již bylo
vysvětleno, s nárůstem komplexity se zvyšuje riziko přetrénování, zejména při
nedostatku trénovacích dat. Vzhledem k množství trénovacích dat, které jsou k
dispozici, se tak dá předpokládat, že trénování této sítě bude spíše méně
stabilní než v případě sítě GRU.

Implementace sítě LSTM se od GRU liší pouze použitím modulu $nn.LSTM$ místo
$nn.GRU$. Použití těchto modulů je v PyTorch velice podobné, pouze LSTM vrací
kromě skrytého stavu také stav buňky (dlouhodobou paměť), ten ale stejně nebude
použit. Pro síť LSTM bude laděna stejná množina hyperparametrů, jako pro GRU
síť, aby bylo možné porovnat jejich výkonnost.

\section{Návrh architektury a konfigurace sítě}

Dále je proveden samotný návrh architektury a konfigurace sítě. Vybrané
architektury byly natrénovány v různých konfiguracích a celý postup trénování
byl zapisován pomocí logovacího nástroje TensorBoard. Nyní na základě grafů
základních metrik, jako jsou ztrátová funkce či přesnost, bude zhodnocen výkon
vybraných architektur. Zároveň bude analyzován vliv jednotlivých hyperparametrů
na proces trénování a výsledný výkon. Nakonec vybereme nejoptimálnější
variantu.

K hyperparametrům, které se zkoušely, patří počet vrstev a jejich velikost a
velikost dropoutu. U dopředných sítí byly navíc zkoušeny i různé aktivační
funkce, u rekurentních sítí pak velikost skrytého stavu. Pro optimalizaci váh
během trénování byl použit optimalizační algoritmus Adam \cite{adam}, jehož
výhodou je rychlost a efektivita.

\subsection{Dopředná neuronová síť}

Nejprve bylo zkoušeno trénování dopředné neuronové sítě. Zkoušelo se od dvou do
čtyř vnitřních vrstev, ve velikostech od $32$ do $512$ neuronů, vždy mocniny
dvou. Nyní budou postupně rozebrány jednotlivé hyperparametry a jejich vliv na
výkon modelu.

U dopředných sítí se nejlépe ověřila velikost dávky $4096$. Síť se lépe
trénovala s použitím normalizace (modul $nn.BatchNorm1d$) a dropoutu. Pro
většinu konfigurací byla velikost dropoutu $0.4$ nejoptimálnější.

Síť byla trénována s těmito vybranými aktivačními funkcemi: $ReLU$, $Tanh$,
$PReLU$ a $Mish$. $ReLU$ a $PReLU$ často dosahovaly podobných výsledků,
většinou ale nejlepších výsledků dosahovala $ReLU$. Příkladem je graf přesnosti
pro trénování sítě se třemi vnitřními vrstvami velikosti $128$, $64$, a $32$ na
obrázku \ref{graph:fnnactivations}.

\begin{figure}[]
    \centering
    \caption{Graf přesnosti na validačních datech v průběhu trénování dopředných sítí s různými aktivačními funkcemi}
    \label{graph:fnnactivations}
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={Epochy},
                ylabel={Validační ztráta},
                ymin=70,
                grid=both,
                width=0.9\textwidth,
                height=0.4\textheight,
                legend pos=south east,
            ]
            \addplot[
                color=blue,
                mark=*,
                no markers
            ]
            table {Plots/relu.dat};
            \addlegendentry{ReLU}
            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/prelu.dat};
            \addlegendentry{PReLU}
            \addplot[
                color=orange,
                mark=*,
                no markers
            ]
            table {Plots/tanh.dat};
            \addlegendentry{Tanh}
            \addplot[
                color=purple,
                mark=*,
                no markers
            ]
            table {Plots/mish.dat};
            \addlegendentry{Mish}
        \end{axis}
    \end{tikzpicture}
\end{figure}

Obecně větší sítě dosahovaly lepších výsledků – menší ztrátové funkce a vyšší
přesnosti na validačních datech, v jistém momentě již ale se ale začalo narážet
na problém se stabilitou trénování. Nejoptimálnějších výsledků dosáhla síť se
třemi vrstvami o velikostech $128$, $64$ a $32$. Jak lze vidět na grafu
\ref{graph:deepffnn}, sítě s větší šířkou nebo hloubkou sice dosahují lepších
výsledků, začínají ale brzy výrazně kolísat. V těchto případech byla zkoušena i
L2 regularizaci (parametr $weight\_decay$), to ale neřešilo problém. Pro
komplexnější sítě by pravděpodobně bylo potřebné větší množství dat.

Nejoptimálnější konfigurace tedy je tato: tří vrstvy o velikostech $128$, $64$
a $32$, dropout $0.3$, normalizaci mezi vrstvami, velikost dávky 4096 a
aktivační funkci $ReLU$. Bylo tak dosaženo validační ztráty $0.18$ a validační
přesnosti 93.8%

\begin{figure}[]
    \centering
    \caption{Graf validační ztráty v průběhu trénování hlubších dopředných sítí }
    \label{graph:deepffnn}
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={Epochy},
                ylabel={Validační ztráta},
                grid=both,
                width=0.9\textwidth,
                height=0.5\textheight,
                legend pos=north east,
                ymax=0.25,
            ]

            \addplot[
                color=blue,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 128, 64, 32}
            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,256,128,32].dat};
            \addlegendentry{Velikosti vrstev: 256, 128, 32}
            \addplot[
                color=orange,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,256,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 256, 128, 64, 32}
            \addplot[
                color=purple,
                mark=*,
                no markers
            ]
            table {Plots/fnn_[34,512,128,64,32].dat};
            \addlegendentry{Velikosti vrstev: 512, 128, 64, 32}
        \end{axis}
    \end{tikzpicture}
\end{figure}

\subsection{GRU síť}

Jak již bylo zmíněno, v oblasti jednodušších RNN jsou dnešním standardem GRU
sítě, sítě LSTM se používají zejména, pokud si GRU s problémem neradí, anebo je
vzhledem k problému důležité uchování dlouhodobých závislostí.

Stejná taktika byla zvolena i zde. Nejdříve bude otestována GRU síť, a to pro
několik možností délky analyzované sekvence. To znamená, že pro každý snímek
bude síti předána očekávaná třída a sekvence póz dané osoby z $n$ posledních
snímků. Následně bude testováno předávání celé sekvence póz dané osoby.

Testovaná GRU síť se skládá z několika vrstev GRU, konkrétně bylo testováno
trénování s 1 až 3 vrstvami, které následuje jedna plně propojená vrstva a
výstupní vrstva. Velikosti GRU vrstev byla zvolena v rozsahu od 64 do 256, u
plně propojené vrstvy bylo zvoleno mezi 32 a 128 neurony. Většinou byly použity
dávky o velikosti $4096$, menší dávky velice destabilizovaly trénování.

Zjistilo se, že na rozdíl od dopředných sítí, v případě GRU nejlepších výsledků
dosahovaly jednodušší sítě. Většina sítí s jednou GRU vrstvou tak dosahovala
poměrně stabilních výsledků. Nejvyšší přesnosti bylo dosaženo u sítě s jednou
GRU vrstvou o velikosti $64$ a plně propojenou vrstvou o velikosti $64$, s
dropoutem 0.4 v plně propojené vrstvě. Co se týče dropoutu mezi GRU vrstvami,
bylo u hlubších sítí většinou dosaženo stabilnějších výsledků s větší hodnotou
dropoutu jako je 0.4, u jednovrstvých GRU sítí je dropout nerelevantní, jelikož
se aplikuje mezi vrstvami.

Sítě s více GRU vrstvami většinou než dosáhly optimálního výkonu, začaly
kolísat a ve validační ztrátě a přesnosti se objevovaly obrovské výkyvy. Byly
zkoušeny větší dávky – $8162$, které sice stabilizovaly trénování při stejném
počtu epoch, jelikož se ale větší dávka spojuje s pomalejším učením,
nedosahovaly takové přesnosti jako při dávkách velikosti $4096$. Pro dosažení
podobné přesnosti tedy bylo vyzkoušeno trénování ve více epochách, tehdy ale
brzy docházelo k přetrénování.

RNN sítě obecně můžou přijímat sekvence libovolné délky, je ale efektivnější,
pokud je síť trénovaná na jednotné délce, tato délka sekvence je pak použitá i
v případě inferencí. Bylo tedy testováno trénování sítě na sekvencích o délce
$50$ a $100$ snímků a s neomezenou délkou – modelu předáváme klíčové body z
celé sekvence snímků, na kterých byla daná osoba detekována. Obecně se ale
zjistilo, že vždycky dosahují stabilnějších výsledků sítě spíše s kratšími
sekvencemi. Pro další práci tedy byla zvolena délka sekvence $50$.

Nejlepší konfigurací GRU sítě je tedy jedna GRU vrstva o velikosti $64$,
následována plně propojenou vrstvou s $64$ neurony a dropoutem 0.4. Trénována
byla na sekvencích o délce $50$ snímků, s velikostí dávky $4096$ a s použitím
optimalizace Adam. Na obrázku \ref{graph:gru} pak lze vidět, že bylo dosaženo
trénovací ztráty $0.066$, validační ztráty $0.211$ a přesnosti $93.12\%$ .

\begin{figure}[] % 'htbp' controls figure placement (here, top, bottom, page)
    \centering
    \caption{Graf trénovací a validační ztráty (nahoře) a graf validační přesnosti v průběhu trénování výsledné sítě}
    \label{graph:gru}
    \begin{tikzpicture}
        \begin{axis}[
                % xlabel={Epochy},
                ylabel={Ztráta}, grid=both, width=0.9\textwidth, height=0.2\textheight, legend
                pos=north east, ymax=0.4, ]

            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/gru_50_1_64_64_0.15_0.4_tloss.dat};
            \addlegendentry{Trénovací ztráta}
            \addplot[
                color=orange,
                mark=*,
                no markers
            ]
            table {Plots/gru_50_1_64_64_0.15_0.4_loss.dat};
            \addlegendentry{Validační ztráta}

        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
        \begin{axis}[
                xlabel={Epochy},
                ylabel={Validační přesnost},
                grid=both,
                width=0.9\textwidth,
                height=0.3\textheight,
                legend pos=north east,
                ymin=65,
            ]

            \addplot[
                color=green,
                mark=*,
                no markers
            ]
            table {Plots/gru_50_1_64_64_0.15_0.4_acc.dat};
            % \addlegendentry{Validační přesnost}

        \end{axis}
    \end{tikzpicture}
\end{figure}

\subsection{LSTM síť}

LSTM sítě byly trénovány se stejnými hyperparametry a konfiguracemi jako sítě
GRU. Obecně se také dá říct, že měly tyto parametry u obou architektur velice
podobný vliv na výsledky, a tedy to, co bylo napsáno o GRU sítích, by se
většinou dalo napsat i o LSTM sítích. Také měly nejlepší výsledky s jednou
vrstvou, podobně se projevovala délka sekvence, a obdobně byl většinou
optimální dropout velikosti $0.4$.

Obecně ale LSTM sítě byly mnohem méně stabilní, a prakticky v žádné konfiguraci
se nepodařilo natrénovat síť do požadované přesnosti, aniž by došlo k
přetrénování anebo by začala validační ztráta extrémně kolísat. Pravděpodobně
tedy není k dispozici dostatečně velký vzorek trénovacích dat pro trénování
LSTM sítě. Také, sítě LSTM nejsou vhodny pro každou úlohu, spíše vynikají pro
delší sekvence.

\section{Testování}
\label{sec:testing}

Vybrané klasifikační modely nakonec byly otestovány na testovacím sadě (tedy
datech vytvořených z testovacích videí, viz sekce \ref{sec:TrainingDataPrep}).
Nyní bude provedena analýza jejich přesnosti a doby inference. Konkrétně byly
testovány dopředná neuronová síť a GRU síť, které v další kapitole, až bude
naimplementovaný celý detektor pádu, budou otestovány na videích z testovací
sady.

\begin{table}[htbp]
    \centering
    \caption{Výsledky testování dopředné a GRU sítě}
    \label{tab:testing}
    \begin{tabular}{|c|c|c|cc|}
        \hline
        \multirow{2}{*}{} & \multirow{2}{*}{Ztráta} & \multirow{2}{*}{Přesnost} & \multicolumn{2}{c|}{Průměrná doba inference [ms]}         \\ \cline{4-5}
                          &                         &                           & \multicolumn{1}{c|}{  CPU  }                      & GPU   \\ \hline
        Dopředná síť      & 0.148                   & 94.43                     & \multicolumn{1}{c|}{0.207}                        & 0.647 \\ \hline
        GRU síť           & 0.138                   & 96.88                     & \multicolumn{1}{c|}{0.258}                        & 0.613 \\ \hline
    \end{tabular}
\end{table}

V tabulce \ref{tab:testing} lze vidět, že obě sítě dosahují velmi dobrého
výkonu. Dopředná síť dosáhla přesnosti $94.43\%$ a ztráty $0.148$, zatímco GRU
síť dosáhla jak lepší přesnosti $96.88\%$, tak trošku menší ztráty $0.138$. Obě
sítě by tak mohly být použity pro detekci pádu, jelikož je ale pro klasifikační
úlohy v konečném řešení důležitější přesnost, GRU síť vypadá jako lepší
kandidát.

Z pohledu rychlosti jsou obě sítě natolik jednoduché, že ani nelze zaznamenat
výrazný rozdíl. Zároveň je velmi zajímavé, že je model na CPU asi třikrát
rychlejší než na GPU. Je to způsobeno tím, že je model malý, nelze tedy plně
využít potenciál paralelismu, navíc zpracováváme výstupy po jednom. Režie GPU
tedy převáží nad jejími výhodami. Ve všech případech se ale u doby inference
jedná o zlomky milisekund, ve výsledném programu doba prakticky zanedbatelná.

V další kapitole bude zkompletován detektor pádu, tedy program, který spojí obě
fáze detekce. Pomocí něj pak budou tyto dva modely vyzkoušeny na videích,
zejména s ohledem na robustnost a stabilitu detektoru.
%
%
%
% TODO ::(
%
%
%
%
%
%
%
%
%
%
%
%
%

% \endinput
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% %
% \subsection{Vícetřídní klasifikace}

% Jak již bylo zmíněno (\ref{sec:TrainingData}), pro naše řešení potřebujeme
% detekovat pouze třídy \textit{normální} a \textit{upadl}, vyzkoušíme ale
% natrénovat i modely se třemi třídami – přidáme třídu \textit{padá}. Nemá smysl
% v našem případě implementovat takovou dopřednou neuronovou síť, jelikož
% klasifikuje snímky bez kontextu předchozích snímků.

% Budeme tedy navíc implementovat sítě GRU a LSTM, s velice podobnou topologií
% jako sítě s binární klasifikací. Hlavním rozdílem bude velikost výstupní
% vrstvy, jež bude rovna počtu tříd a použitá ztrátová funkce – nyní použijeme
% křížovou entropii. Výstup vícetřídní klasifikační sítě pak představuje pole
% pravděpodobností, že se jedná o danou třídu. Při inferenci pak použijeme
% jednotku softmax pro převod výstupu na číslo třídy.