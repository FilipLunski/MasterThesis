\chapter{Implementace detekčního algoritmu}
\label{chap:detectionAlgorithm}

V této kapitole se zaměříme na implementaci samotného detekčního algoritmu,
který na základě postupně předávané sekvence snímků bude detekovat, zda došlo k
pádu.

\section{Sledování pózy s YOLO11}

V YOLO verze 11 máme kromě již zmíněné detekce objektů i klíčových bodů k
dispozici také další volitelné funkce. To, které funkce chceme využít,
definujeme vybraným modelem. V závislosti na něm pak při interferenci model
vrací patřičné hodnoty. Dostupné jsou tyto modely:
\begin{itemize}
    \item \textit{YOLO11<v>-seg }- detekce objektů - bounding boxů
    \item \textit{YOLO11<v>-cls }- detekce objektů, klíčových bodů a segmentace
    \item \textit{YOLO11<v>-pose} - detekce klíčových bodů
    \item \textit{YOLO11<v>-obb }- orientovaná detekce objektů - bounding boxy natočené dle natočení objektů
\end{itemize}

kde $<v>$ označuje velikost modelu - můžeme vybrat menší modely pro větší výkon
ale horší přesnost, anebo větší modely, které jsou sice velmi přesné, musíme
ale počítat s vysokými nároky na výkon. V našem případě jsme zvolili model
$YOLO11m-pose$. Pro inicializaci modelu musíme specifikovat cestu k němu, pokud
není nalezen, knihovna automaticky stáhne předtrénovaný model.

\begin{lstlisting}[language=Python, label=src:params, caption={Inicializace modelu $YOLO11m-pose$}]    
    from ultralytics import YOLO    
    pose_model = YOLO("./models_pose/yolo11m-pose.pt")
\end{lstlisting}

Dále můžeme každý z těchto modelů používat v několika režimech. Režim
specifikujeme tím, jakou metodu modelu použijeme. Pokud chceme model natrénovat
na vlastních datech, použijeme režimy \textit{trénování} (metoda $train()$) a
\textit{validace} (metoda $val()$). Pokud chceme používat již natrénovaný
model, máme k dispozici dva režimy: \textit{predikce} (přímé použití modelu,
např. $pose_model()$) pro vyhodnocení každého vstupního snímku zvlášť a
\textit{sledování} (metoda $track()$).

Režim \textit{sledování} vrací stejné informace jako režim \textit{predikce},
navíc ale sleduje pohyb objektů mezi jednotlivými snímky a přiřazuje jim $id$.
V našem případě tedy kromě bounding boxu a klíčových bodů dostaneme i $id$
každé osoby.

YOLO11 dokáže zpracovat širokou škálu vstupních dat, včetně videí, kdy nám je
vráceno pole výsledků, datových proudů (je třeba specifikovat příznak
$stream$), kdy nám je vrácen iterátor, nebo obrázků, kdy dostaneme výsledek pro
daný snímek. Pokud chceme použít režim \textit{sledování} na jednotlivé snímky,
musíme specifikovat příznak $persist$, který zajistí, že mezi jednotlivými
snímky bude uchovávána informace o sledovaných objektech. Zde můžeme vidět
použití metody $track()$ v našem programu (příznaky $show$ a $verbose$
specifikují, zda se má zobrazit výsledek každého snímku a zda má model do
konzoly vypisovat ladící informace)

\begin{lstlisting}[language=Python, label=src:params, caption={Použití sledování pomocí YOLO11}]
    results = self._pose_model.track(
        frame, show=False, verbose=False, persist=True)
\end{lstlisting}

\section{Struktura třídy FallDetector}

Třída $FallDetector$ slouží pro detekci pádu v sekvenci postupně předávaných
snímků. Můžeme ji tak použít jak pro analýzu uloženého videa, tak pro detekci
pádu v živém přenosu v reálném čase. Mezi jednotlivými snímky uchovává pro
každou osobu klíčové body z $x$ posledních snímků pro predikci na základě
sekvence snímků. Dále dle nastavení uchovává $n$ posledních snímků, popřípadě i
anotovaných, pro uložení videa včetně několika sekund před a po pádu. Pro
ukládání těchto informací používáme kolekci typu $deque$, která umožňuje
specifikovat její maximální délku, nerelevantní informace ze starších snímků
tedy budou automaticky odstraňovány.

V konstruktoru obdrží tato třída cestu k modelu YOLO (musí být inicializován
pro každou instanci zvlášť), model pro detekci pádu na základě klíčových bodů,
informaci o délce sekvence předávané tomuto modelu, a informace týkající se
ukládání videí s pády, jako jsou cesty k souborům a délka videa před a po pádu.

Pro práci s jednotlivými osobami používáme třídu $Person$, která uchovává
hlavně klíčové body z posledních snímku a $id$, dále také bounding boxy,
detekovaný stav a confidence score této detekce pro poslední snímek, kdy byla
osoba detekována. V případě, že daná osoba již není detekována, obsahuje
informaci, kolik snímku již není vidět pro vymazání osob, jež odešly ze scény.
Okamžité vymazání zmizelé osoby není žádoucí, jelikož někdy osoba je pořád na
scéně, ale algoritmus ji i na několik snímku ztratí, zejména v případě okluze
či špatných světelných podmínek.

V metodě $process_frame$ třídy $FallDetector$ je zpracován vždy jeden snímek.
Nejprve se detekují všechny osoby a jejich klíčové body pomocí vybraného
modelu. Pokud je požadováno uložení anotovaného videa, převezmeme zde anotovaný
snímek přímo od modelu YOLO, později pouze dopíšeme třídu detekce pádu. Dále
jsou na základě detekovaných informací vytvořeny, popřípadě aktualizovány
instance třídy $Person$. Pro každou osobu je také provedena analýza sekvence
klíčových bodů pro detekci pádu a popřípadě je patřičně anotována ve snímku.
Pokud je detekován pád a je nastaveno ukládání videa, vytvoří se nový soubor a
zapíše se do něj uchované předešlé snímky. Do souboru se pak zapisuje, dokud je  
detekován pád a nevyprší požadovaný počet snímků po pádu.

\endinput